{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a67c90",
   "metadata": {},
   "source": [
    "# Многослойные нейронные сети. Решение прикладных задач обработки данных на нейронных сетях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6518f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "# import keras.models as M\n",
    "# import keras.layers as L\n",
    "\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abb7e7",
   "metadata": {},
   "source": [
    "## Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ca6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white = pd.read_csv(\"winequality-white.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aafafae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68be4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "# размер датасета и количество признаков\n",
    "df_white.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e81413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выходная переменная\n",
    "df_white['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac11f2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# наличие пропусков\n",
    "df_white.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b207db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2       True\n",
       "3       True\n",
       "4       True\n",
       "        ... \n",
       "4893    True\n",
       "4894    True\n",
       "4895    True\n",
       "4896    True\n",
       "4897    True\n",
       "Length: 4898, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# наличие повторов\n",
    "df_white.duplicated(['quality'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7251463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showGistogram(df):\n",
    "#     plt.clf()\n",
    "#     plt.figure(figsize = (16, 10))\n",
    "    \n",
    "#     data_size = df.shape[0]\n",
    "#     k = round(1 + np.log2(data_size))\n",
    "    \n",
    "#     fontsize = 12\n",
    "#     tick_labelsize = 8\n",
    "        \n",
    "#     for i in range(0, len(df.columns) - 1):\n",
    "#         plt.subplot(3, 4, i + 1)\n",
    "#         plt.title(df.columns[i], fontsize=fontsize)\n",
    "#         plt.hist(df[df.columns[i]], k, edgecolor='black', linewidth=0.1)\n",
    "#         plt.tick_params(labelsize=tick_labelsize) \n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"./Graphs/Data/Distribution.jpg\")\n",
    "#     plt.show()\n",
    "    \n",
    "# showGistogram(df_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d46f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showBox(df):\n",
    "#     plt.clf()\n",
    "#     plt.figure(figsize = (16, 10))\n",
    "    \n",
    "#     fontsize = 12\n",
    "#     tick_labelsize = 8\n",
    "    \n",
    "#     for i in range(0, len(df.columns) - 1):\n",
    "#         plt.subplot(3, 4, i + 1)\n",
    "#         plt.title(df.columns[i], fontsize=fontsize)\n",
    "#         plt.boxplot(df[df.columns[i]])\n",
    "#         plt.tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"./Graphs/Data/BoxAndWHhisker.jpg\", bbox_inches='tight')           \n",
    "#     plt.show()\n",
    "    \n",
    "# showBox(df_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8c640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# fig, axes = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# tick_labelsize= 10\n",
    "\n",
    "# cols = list(df_white.columns)\n",
    "# cols.pop(len(cols)-1)\n",
    "# hm = sns.heatmap(df_white[cols].corr(), cbar=True, annot=True)\n",
    "\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.savefig(f\"./Graphs/Data/Correlation.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f44009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showScatters(df):\n",
    "#     plt.clf()\n",
    "#     plt.figure(figsize = (10, 10))\n",
    "\n",
    "#     tick_labelsize= 10\n",
    "\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     plt.xlabel('fixed acidity')\n",
    "#     plt.ylabel('volatile acidity')\n",
    "#     plt.scatter(df['fixed acidity'], df['volatile acidity'])\n",
    "\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.xlabel('chlorides')\n",
    "#     plt.ylabel('alcohol')\n",
    "#     plt.scatter(df['chlorides'], df['alcohol'])\n",
    "    \n",
    "#     plt.subplot(2, 2, 3)\n",
    "#     plt.xlabel('citric acid')\n",
    "#     plt.ylabel('chlorides')\n",
    "#     plt.scatter(df['citric acid'], df['chlorides'])\n",
    "\n",
    "#     plt.subplot(2, 2, 4)\n",
    "#     plt.xlabel('volatile acidity')\n",
    "#     plt.ylabel('alcohol')\n",
    "#     plt.scatter(df['volatile acidity'], df['alcohol'])\n",
    "\n",
    "#     plt.tick_params(labelsize=tick_labelsize) \n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     plt.savefig(f\"./Graphs/Data/Scattering.jpg\", bbox_inches='tight')\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "# showScatters(df_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24afc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526b4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white = df_white.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e75acd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3961, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0131d202",
   "metadata": {},
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7261295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# типо удаление выбросов\n",
    "\n",
    "filt_df = df_white.loc[:, df_white.columns != 'quality']\n",
    "\n",
    "low = .01\n",
    "high = .99\n",
    "quant_df = filt_df.quantile([low, high])\n",
    "#print(quant_df)\n",
    "\n",
    "filt_df = filt_df.apply(lambda x: x[(x >= quant_df.loc[low,x.name]) & \n",
    "                                    (x <= quant_df.loc[high,x.name])], axis=0)\n",
    "\n",
    "filt_df = pd.concat([filt_df, df_white.loc[:,'quality']], axis=1)\n",
    "filt_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba72540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3371, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b632f167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# showGistogram(filt_df)\n",
    "# showBox(filt_df)\n",
    "# showScatters(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae26596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>0.064584</td>\n",
       "      <td>-0.936791</td>\n",
       "      <td>0.271723</td>\n",
       "      <td>-1.361716</td>\n",
       "      <td>-0.138380</td>\n",
       "      <td>0.087359</td>\n",
       "      <td>0.741858</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>-0.932970</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.059021</td>\n",
       "      <td>0.618562</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.334065</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-1.025751</td>\n",
       "      <td>0.502191</td>\n",
       "      <td>0.451354</td>\n",
       "      <td>-0.484375</td>\n",
       "      <td>-0.414378</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2</td>\n",
       "      <td>-0.504606</td>\n",
       "      <td>-0.120075</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.833316</td>\n",
       "      <td>1.230708</td>\n",
       "      <td>0.690750</td>\n",
       "      <td>-0.057028</td>\n",
       "      <td>-0.876763</td>\n",
       "      <td>-0.587242</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.509923</td>\n",
       "      <td>-1.597349</td>\n",
       "      <td>0.253252</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-0.036966</td>\n",
       "      <td>0.426767</td>\n",
       "      <td>-0.129654</td>\n",
       "      <td>-0.190083</td>\n",
       "      <td>-0.846538</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.1</td>\n",
       "      <td>-0.617331</td>\n",
       "      <td>0.895551</td>\n",
       "      <td>-0.958829</td>\n",
       "      <td>-0.039983</td>\n",
       "      <td>-0.430490</td>\n",
       "      <td>-0.214440</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>-0.386277</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>5.7</td>\n",
       "      <td>-0.730057</td>\n",
       "      <td>-0.120075</td>\n",
       "      <td>-1.091056</td>\n",
       "      <td>-0.414030</td>\n",
       "      <td>0.234671</td>\n",
       "      <td>-0.417268</td>\n",
       "      <td>-1.142051</td>\n",
       "      <td>0.306102</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-0.504606</td>\n",
       "      <td>0.433903</td>\n",
       "      <td>-1.002904</td>\n",
       "      <td>-0.788077</td>\n",
       "      <td>-0.363974</td>\n",
       "      <td>-0.645449</td>\n",
       "      <td>-0.297303</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.496597</td>\n",
       "      <td>-0.760106</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>6.2</td>\n",
       "      <td>-0.730057</td>\n",
       "      <td>-0.397064</td>\n",
       "      <td>-0.936791</td>\n",
       "      <td>-0.351689</td>\n",
       "      <td>-0.696555</td>\n",
       "      <td>-1.152519</td>\n",
       "      <td>-0.991203</td>\n",
       "      <td>0.523980</td>\n",
       "      <td>0.104208</td>\n",
       "      <td>0.536374</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.509923</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.147041</td>\n",
       "      <td>1.498477</td>\n",
       "      <td>0.774345</td>\n",
       "      <td>0.426767</td>\n",
       "      <td>-0.347532</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>-0.846538</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-0.391881</td>\n",
       "      <td>-1.320360</td>\n",
       "      <td>-1.024942</td>\n",
       "      <td>-0.227006</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-0.670803</td>\n",
       "      <td>-0.463236</td>\n",
       "      <td>-1.509548</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>-1.019402</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3371 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.3          0.284472     0.064584       -0.936791   0.271723   \n",
       "1               8.1          0.059021     0.618562        0.231214   0.334065   \n",
       "2               7.2         -0.504606    -0.120075        0.583819   0.832794   \n",
       "3               6.2          0.509923    -1.597349        0.253252   0.022359   \n",
       "4               8.1         -0.617331     0.895551       -0.958829  -0.039983   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "3366            5.7         -0.730057    -0.120075       -1.091056  -0.414030   \n",
       "3367            6.5         -0.504606     0.433903       -1.002904  -0.788077   \n",
       "3368            6.2         -0.730057    -0.397064       -0.936791  -0.351689   \n",
       "3369            6.6          0.509923     0.249243        0.473630   0.147041   \n",
       "3370            6.5         -0.391881    -1.320360       -1.024942  -0.227006   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -1.361716             -0.138380  0.087359  0.741858   \n",
       "1               -0.297458             -1.025751  0.502191  0.451354   \n",
       "2                0.833316              1.230708  0.690750 -0.057028   \n",
       "3               -0.297458             -0.036966  0.426767 -0.129654   \n",
       "4               -0.430490             -0.214440  0.011935  0.160850   \n",
       "...                   ...                   ...       ...       ...   \n",
       "3366             0.234671             -0.417268 -1.142051  0.306102   \n",
       "3367            -0.363974             -0.645449 -0.297303  0.669232   \n",
       "3368            -0.696555             -1.152519 -0.991203  0.523980   \n",
       "3369             1.498477              0.774345  0.426767 -0.347532   \n",
       "3370            -0.297458             -0.670803 -0.463236 -1.509548   \n",
       "\n",
       "      sulphates   alcohol  quality  \n",
       "0      0.006111 -0.932970      6.0  \n",
       "1     -0.484375 -0.414378      6.0  \n",
       "2     -0.876763 -0.587242      6.0  \n",
       "3     -0.190083 -0.846538      6.0  \n",
       "4     -0.386277  0.363510      6.0  \n",
       "...         ...       ...      ...  \n",
       "3366  -0.288180  0.017782      6.0  \n",
       "3367   0.496597 -0.760106      5.0  \n",
       "3368   0.104208  0.536374      6.0  \n",
       "3369  -0.288180 -0.846538      5.0  \n",
       "3370  -0.288180 -1.019402      6.0  \n",
       "\n",
       "[3371 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standarting values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_values_ = filt_df.values\n",
    "data_values = data_values_[:,1:-1]\n",
    "data_values = scaler.fit_transform(data_values)\n",
    "\n",
    "data_values_[:,1:-1] = data_values\n",
    "filt_df_norm = pd.DataFrame(data_values_)\n",
    "filt_df_norm.columns = filt_df.columns\n",
    "filt_df_norm[filt_df_norm.columns[1:]] = filt_df_norm[filt_df_norm.columns[1:]].apply(pd.to_numeric)\n",
    "filt_df_norm"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABE8AAAGeCAYAAABl+C86AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAJcrSURBVHhe7d177CVlneD/6t0/hHYAuSgGkIuzLDhRIMRxQWQYQRAMKypZrmIUNkp0EEFYNQbHQIy6kh+DRqMkKNkWhE5QcAnKIE6IICwxDOJMhGEGENAVw0VhaNi/+tfv6vP58vTTdZ6qOt9z+lvn9PuVnO5zqVOn6rnXp56q76r1G1SSJEmSJElq9B9G/0uSJEmSJKmBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBVs9cGTO++8s1q1alX9OPbYY6tvfetb9fOrr756tMT0PP300/VvnH322dWLL744enc6Lrnkknq72Z8msZ+xX/nybM//+B//o3rwwQfr10PUto85luM7iLTnwXP286CDDppJXpS0/S7v8VlpO9P9WgmUobyO8LxUZ/L031K6/G7sT9dytSX0LSfL1ZSnW5u2NJ9WGe7bVq+E2AYe0ypjs5Sn6Zba/rTtmFb5CG11Mn5vXHmdFvatb3/DNnctv231YTnyNIw0471Z/Sai/MXvTnMft3Tf0MeWKpMY4tioi7xMhigzK7n947ZtnL7LN+G7y/n+LLXVtb6W2w5Me3tmIdqAaH/mtZ622eqDJz/96U+rAw88sHrggQeqH//4x9VHPvKRav369dXpp58+WmI+XHTRRfV2H3bYYaN3ytLlKdDnnXde9fd///ejT+cflfNtb3vb6FVV7bzzznX+8uD5Stlvv/2q++67r/rmN79ZbbvttqN3x8uXz/drJVA30jpCx/f+97+/fj7OSqX/UPK9r77lRPOjb1u9EvJ+cZ7qDlZi+6fd1uTt7EqYpL/p0h+sFMrDLbfcUn33u9/donVwmnXevmGjIY6N1M+Q2wpNx6LW0602eELAgEjY5z73ueqXv/xltf/++9eRMCpzRFLTsxRkOCJyyOeIqBrvpe+HWJ7HHXfcMXq3Wfp78YjfRf55GrmL34nl02X5/6mnnqrfD7E8g0wCJ8y4iXT427/9283Wz3rz9/qIdEq/3xSRjO3iwWcsM05sUzzYT/ab96Nykr+8/9BDD9X/xzJNIu+7/Ha6LI90HzBuXU37nC67Zs2a+r2QLk9epft19NFHVx/84Ac3WX+U67btbxPrie1q2l7+5xGdH/+zHOWI3z/11FPrB8tSvvL0L5Xn3Li8DvnnvEb8Rrp8pCnLsb0vvPBC/f5yRJowe4t1R/qnv8WD5UK+//EdxPea0p1HqZzky0datKVhSf7dceUBUXbGpTn53GfbYtk8bScV6+Mxbl201bFM7FeT2NdYNt3uyF8esQz7Hm1c7HssF9/P22qk7WL6G0j3h0epHrWJ/cn7xfj9+D+2oS0tS9udytORR54+6fejvOR58//+3/9r3P7Yzlg+L6NRPtM2K1830nLM99O2o2k7S+Uj9iHei+9HOubbjDQ9m8YT8R0e48p2V2xf3o/Gtqfbkf4Ov5/3B6RBul084v0u8u/y2yG2Iy8rTetnPbE/bBvL/Z//83/q7U+Xj99jnbG+WGf++yGW43P+Hzfeiu3EuDRM1xXLx7JsW5TBpm3mkfcNiO/EMiw/Lel+9CmTadpedtllS8uk6ZumRf79NB225NioTaTHFVdcUf8Wz9kH9mVLSNObR5qesW1d6gv67Essm6dvvj3xW7zf1FZEvsbyaVktlYdJ5evkkdbTHJ+Vlo104DFu+9atW1fvL8vk6Znvf5p/05LuM79F/vI80rqtnERdim1Mv5uL/WH5IdXT5fKeJwWcPeKMAeiQbr311jrDmZ3yvve9r87Yk08+uR6kBRqBKHAUJpYP73nPe+qzHk0oMJ/97Gc3+5xCTMHlwRmo9HPW3VSx8nXxP7/d1W677Va9853vrPcjKjWFHkceeWT9f1977rlndcghh2yyzl/84hd12h1++OF1RJJ9SdMrBr954wTS/mMf+9jo1Ubs5ze+8Y3Rq37Shhz89rnnnru0rSm2J10WBAe+//3v18+b1kU5aWoI8nVRtlhXF//xP/7H6tBDD63XT1riscceq+6+++46rUnzSVB+IqAWeM57fNbVtddeWz84A0xDmOpTntvymjSMBjnwuqnc8LvkK2kG9os0n5avfOUr9bpJ/5deemls+9BU31mua5nrU06wnPrS9F1++9JLLx29KsvTnHz+3//7f9fP0XXb0rSdtGx3qZvs2zHHHDN6tfF3089DUz1hu4866qhNlue9WCZvP7u01Xm7yDJ8h+/m5QIsO24gs1yxHZxBZ4ZFnpZp+S1td46ylJdnykRTus9S2ma9+c1vHr27Ud+2o618kIYXX3xx/R5pSXnn+YUXXlifrcu1jSf69GHL0aefxl133bVZGSVNor8s2dLlexzSeTl1OFdKw7ZxZ66tb6CslcapyzGNMsny559//ujVxvxl2/J0RdcyPauxUR/kQ6Q5+zCu7ZumWdWXtn3hN6McsBzBMD5v2p5S3f+Xf/mXqY2ZumhaJ6ifTetkO0pjza5tI+OLqKP8dox1muoq64ugxTTk+8xvlfqxJk399bhxUskQ6umkttrgCQfrVHAKTUzvjQ4rFQMcCgoFnmUZEPH9OPgn2s6UTM42EHSgw/vtb3+7dAYgPm+KygfWx7QmluNBZJJt+/3vf1+vl8EVhZ1tSX+LSplX8nvvvbfeXj5nOZbne01e8YpXbJYOPGe/47dZP7/D+mgIJsH+ESRhH/gNKvDPfvazpUEqle7666/fZJsjvUjHvOGIqWCRXqyTdZHuBx988NJ32W/SbqeddqpfN4ltyX+bbeW7OdImfjeWxcMPP1ynFdsbacnnTBFOG4fA70YZiWnEsR9NaGTy/frrv/7renl+G1EmIyA1iSg/lAPKYZRFGjQathQBELYd/E8Z3mabberXkQbk03/6T/+pfi/0Kc+lvH7mmWfG1rMI+KXid/N9m5ZYL+nwT//0T2PbhyjPaZnjwfblU/77lpMmpTTM69Y4kVfxaGovm+Rpzv7+8Y9/HH3afdvStJ2kbHetm+nn7DOfs825vJ6k61u7du1oqY2iDNB2pNra6mh789/gO3w3RLmIx6SXepCupX4xysAFF1xQbDO7bncqTXcelImmIEJJU3/WtZwivtP0233bji7l46Mf/WidhnHwwbJNB8hRdpG3c6Ce9OnDuqCsxm+Q76yHdZf6abadfQT/U1cJNrBcPGL56LO6mEb5zvsq9udVr3pV/boLtpvf7luHc13GOvwG62C9+bgz1aVvKI1Tu7b9TaZZJuP7kW6PPvpo/T/S7/Pgu3kfuaXGRn2wHWxvpHc+fqLOp2fu84Py5ZhGfUm17Uu0cfE5+cfr0li5aew4rTFTV5QDfjfWFW16HPuk0roW2xf7w1izzzFMpGfU1Rjr0C+w/5F/sT2lfrOvCE6k2xn1riv6VL4XD77PdudplhtiPZ2UM086OOmkk5Y6o/SsUGQ2jR6N3y677FJ3DDQcTzzxRF0BKfgczIPAAwW2DdHL1atX1xUmxG/FWQ8aCwpeU8MRHc8ZZ5yx9FnfGSMsTwGmIFMh2BcawkkbKRAkIR1paKICn3jiiXV6RoVJt5l0I/2iIW5CB850MtKW70+CRovfIO/Iw7Qji3RvQmPJjIq004u0SqOmdBI0MHkHFr9LmWAfwHf4blexfAQd2F7SOD9r2keUn2i8ooPpezBTihz3Kc+hKa/TNIzAXnTYTQdN8buUs9g39nNa0gY/fqupfWD7WDYtc+POLkyjnIRJ6kv8Fgd3bCePPmey8jQnf3mea9u25XamXetm+vnrX//6+v8mEZyL/QLbTz4xGGLmEdKymYu6lrZ7aVsd20xfQJ9A2seZPL7Lell/OhCnLZiV2DbaY35/XJvZtt256G/YH5YlHSkPW1qXNqtr29FWPqjXaV2g3W46QEak57jxRLQRffuwvibtp9HUX7bZ0uV7nOXU4VzXNBw37kx16RtK/VApv9pMq0ymaZumW9SvLn1kk0iHaY6NuuJ3yD9EHSfPSbNZmkV96bIv0TfT1u+9996jd1/Wte6XymqfMdMkmo67Umldi/KajjX7tI1RztmP1772tfVzTgSSV2ldJk1ZH1jHNJBvbGd6PFdqr0oYB5IP6Wybvlayni6HwZMOorCBSHpbZaVDefbZZ0evumGdcZ0XDRUVjUq3UqIRZn9vv/32ev8nrWAhrSQ333zzJutsanBLojFm+tkPfvCDzc62TEvTmXm2nwZj7dq1dUAhIqkrIR1gsB1sW+kAYB5tqbzeUmgfaFPiwD2i/nTaRObZ32lIB6jLSUPKGMEztpk2AQzOxg1caLv6dPTzmr+lwMqWQP4y+CHgmKYZA08GgislDRw1SctliEFonImMATLtWZPlDiT7ltFJdC0fkR70hwzAp62pD5tU334ay+kv+5Zv2ija1+VoKp/T1DUN+447+4p+aCW0tRFhOX3kSo6NyLe2tM1nh0xjHDmL+tJlX8aZ1lh5VmOmaR53TdI2piivy13HrOTlhPJEvrK9aX5MYl6PYQyetCASljY8VNaYPhWDIzI8bQRpJP7yL/+yjiimy9OoUUCaxEyMKDT54C5+K6Z/pZU+bziiArIs28+yMd2sKxphBrRsP41JGm2dVFpJuMY1XScDZRr72GbEdFj2h4hwKqK8rI/1UrljoNFXNFpsD+tJ85KDRj5PxRnFCPyk+RSR5DTfowPJO7D4XdKDsoHY5z5iRk9cbxxpMqm8/CAaS/ZlGvqU51JeN6VhHIyzvnyAlv8u+9e3bnRVah/Ss4gxBZLBFPuVH0D1KSeURdoO9ivNq2nUlxiYxQAjn7obBx0M3NjW0Jbm06zLJX3qZhdpPWG/EG387rvvvnT5Wkle11hPmjaxzaQ36Z6Wo3RmFeWJckXa0Y6xX1F3ZyHOMI5rM/fYY49O252LwXEMsqOtRQygSSMOJicxrox20bftaCsflHXyKT1rF+Ug11R2Y12INqJrHzapvv00Sv1lV23lO9YZbUlf49rNLtrqcK5LGvL+uHFnKvKdcjCub+jaD/W13DLZpX0MbX3kONMeG/UR5Z68JK/ZDtJsS5h2fZl0X/rW/WmNmbpqO+5KNdW1dKz5Z3/2Z73bxhTrp19I15+2JaxjGqL9iTJRaq+aykk6Q4bjN75PMHQ5VrKeTsrgSYvoDIisRTSXTo1CFxkeU8ziweccaMR0q/g8pi2XUNGoZBRwfjfEdN/4nAfP47KXVEwV4/usJ5ZtQ+WgMkSnnc40Sad4LUfsB9J1sg/sS2wz6UW6gXQcV5FiemIsm2NgSuP2r//6r6N3Nse6qazpb8ejNJBqytemfOd/yklMfQz8bizbth+52C8abxp9Gn/wO8ud7paXH7aL3+O9mJ7bhH0gf9P7WYzTpzyHpjRqSkPKMGWZ9/MBWv67eT2bplL7QPvBtqTvs/1N+delnLAfDGQjz8bt17jvl1AH0u2MvIoBR3TqlJGmdTeledO9BibZtj761M0uop7EfrG+yMOu62trq9nmCGLHb/CIuh/TZuMR+T6t9nqctjazbbtTDL4YfKbriDJAH8T3KWPRP+Vp1EVbGe2ib9vRVj7SQSsHCxwIsL6mmzg3lV3WFSbtw7qK/gZd+2m2jzTjgARN292mrXzHwRbr7LtusL4u7WZJWx3OdRnrlMadqS59Q6kfWo5Zl8k+fWRqlmOjPqKtiTIV/eUszaq+LHdf2uo+75PX3BtvXFmdtDx00aVNb6pr6ViT7ZjkGCZFv8B6Yv3RlrSNvfuIOhHb2dRedSknpe93MZR6OimDJwVkKncQZsDEjd3STo8BDq+vu+66OrMDhZyb6YGzaOl0pv/v//v/6nU1YV1c1xr4HgMqKiZRPxo+Gsb0++lvpaikX/jCF5aWZftKhTttFFI0DLGONJCyHOwHDTnydRJNTtOL7WbgEMunuDEd+x9IK74bZ8NjQNMVecXgNcXrpt+Om/yBbfzHf/zH+rcYENDAN63r61//emNQgPWny1IGStvdtF8xSME0OmjWFzddDOwvZaqpA4iDij76lOe2vCYNeS81Lu/43csvv3ypzvL71MtZKLUPTfuPScsJ60vP6PNZWufb0rCkKX3T8sDnab2Nzj7kac5+/Nf/+l/r51jOtvXVp262GVdPbrvtts7rYx1tbXXeLiK2uWl/2J6mejRtbW1mabtTTemIWBefcy+QtM7ecMMN9fOu2spoF33bjrbywV+cYDvYLrYv2lEGlU0Hl6R3ug/5eKItPybR1N+09dN5f3DmmWcuvWbZvL8saSvfedtBuUh/u01bu9lFlzqcK6Vh27gzx3fSNMr7Br5fGqcuxyzLZN8+ckuMjfqgDESasw/jxk/TNKv6Mum+tI2V87aC4Mm0xkxd8L3ScVeOclsaa7a1jW2a6irrm+bswby94v+8Py2VE/74Rt5mkq9sczpTdJyh1dNJrVrP3CepQQQCQKNF46VhIn+IDtOQR55JQ8BAiTMwdL7RjnAmiYNEBiJdBxaSJPWxpcdGi9S32U9vHWKMtpLHD/N2DOPME41FxJSzHTSaBk6Gi+nf3AOAyO88THfT1oWpnZTNmObJNFAGZARTmN0mSdK0OTaShm8e66nBE20mrkGPs8VMvdMwETGOaw6Zfjjp9EVpViiTTK1N0UlyCYRBWUnStDk2koZvXuupl+1IkiRJkiQVOPNEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFUz0p4pXrVo1eiZJkiRJkjRbE4Qupmqi4Mm6detGzyRJkiRJkmZr9erVo2crw8t2JEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQVzFTx58cUXq49//OPVK1/5yvrxpS99afSJ2jzzzDPVCSecsJR26eO6664bLbWpu+66a2mZQw45pHrooYdGn2gaSPdIX/KGPBonXTbNizSP0sci5ldehseVW+RtRb5snm6ldamfcWUVXdvwSdqroenTfnYt27Ec6w5NbYDleT6Rb9QP6gny+sKj1Ff0KXMaDvItzfemOp3naZ+87tN39sW6P/jBDxZ/v63d77IvsQ6WTZW+2/a7k8jX2ZaWfB7L5nV3lvkyJH3Kamq57eFQsf+kQ+xDKd/zMrKIx715vs5Dns5N8ITE/dSnPlVdeeWVo3eq6pJLLlnYxmbadtppp+rGG2+sXnjhhaXHt7/97eod73hHdfTRR4+WehmV+4tf/GJ133331ctedtll1QUXXDB3jdRQ0Zlcc8011eOPP16n72mnnVZ9/vOfX+okUpTxdNk0Lw499NCl/OTx1FNPVWeddVb1nve8p9p3331Ha5h/pAvpQzqxn6QFaZIPpBBtxW677da4LGX7vPPOq37yk5+0rkv9lMoqLr/88qV8oaz+7ne/a2zD+7ZXQ9On/exTtq+44oq63KZuv/32pbIcj5NPPnn0qeYFZYZyknriiSeq7bbbrq4rkbfUC+pHzj57PlHPaddSbf36rNqXvvi9D33oQ9UDDzwweqdZqd3vsi/Rp6fjf7R9t2t/01Xb2CLH+2l/mI7zZpkvQzJpu8T3WDbVpz0cKvab/WcM2pbvUUaiPZhGGR4i6ikiX9N6MlRzEzyh0txzzz11B0ICUxHf9KY3VXfeeeegE3ioomH69Kc/3djwXH/99dVb3/rWpQPwgw46qNprr72qBx98sH6tyVFev/e979UNRKQ9B4TPPfdcXc5TLEsZT5ct5cUPf/jD6je/+U314Q9/ePTOYqC+s19x4ExakCYcOOZIw0cffbQ68cQT69csS+cTy1K2GYTyHvicDjheazJtZZU25+c///lSvmy77bbVqaee2qkNb2uvhqZP+9m1bDO4YuCUHmiRbs8//3y1yy67jN7RPCIfr7rqquqoo44avbMRYx0OFqgrbeyz5wsHUZxh5SDqE5/4xOjdZnm/Pov2pS/OgL/uda+r3vjGN1avec1rRu9urq3db9sXDhSjfcuDTKXvLqe/GadtbJHiN0rjvFnly9BM0i5Noz0cqqeffrpav359dfDBB9evoww99thj9esU6UAZOeKII+rX7Pdhhx1WPfLII/XrRRD19JxzzlnKV07+fPWrXx10Ps9N8IRK86tf/aouOCToHnvsUb3lLW+pC9akDeHWjAYtPYBMkZ4M0vfZZ5/ROxsrLdH2RWvYV0I0iHvuuefonY3pu/3221f33nvv6J2NeJ9GJD2THN/PMRgjgj0vB5h90LHQ4aaNKenHAWV+BoNOmsFmdNZ8znJ0QOlzTVdbWaUNX7VqVbXzzjvXr8GgmMFoHjTMldqroenbfnYp2/zPQJwBRorfInhyxhlnLE15XbSzUlsD2qsDDjigfqQoG7/+9a9bpzP3LXMahnPPPbe6++67N8v3FPmd9uuzaF8mQdCENp1LdkpK7f6//du/te7Lq1/96vrsPGeiU23psJz+ZpzS2CLH9pXGebPKlyGZtF1abns4ZJRHymWM9aMMpeUkkFaUkUgr0pPgX5qe84502Hvvvetj+nkyN8ETKk2TJ598so7kqbs8Ij9OXpkXqcKutB133HGTs8XRoXRx66231g0qEfzUuPcXBemTDjRIP9JxHDoarqPk7BgH3XHgHd9JryOl89L05WUyHywykGgL9HVtr4amT/vZVrZJR04cpAcCoO9jRiazctIpwAZQ5geDZwbEcQY6xRnGHXbYoT4QjOnMXCYx7oDBPnt+0O4deeSRo1fjjevXp9m+TOL444/fZJ0lbe1+aV9Io1IfUfruJP1NF+PGFrm2cd4s8mWI+pTVabaHQ0T5u/baa+t9ZOz5rne9q7r00ksbyxBlgxNSYFnKB7OnFu2yXOoBAbMYj6f3uRmqubphrKZjXiN92jiFlQOldIobIiIdM7P0csdDJ0tnHQ3ys88+W085/s53vlN/xvRZpk4bQJmucWW1r629vSJ4dP/991fvfve7R++8jDOgnLmOgRcDM85QE0CZpwHl1oz72DAgbjqoIy/T6cuUAQ4IOaDW4rNfH6ZxYwst36K3h/Tnb3/72+t9pPzcfPPN9T1Qmk540Idzko8yxrIEjZiBumg3jeX+pWAfeRBM4d5CQ65TcxM8ySOXYdddd93sbJzG69MZ57N9Fuk6u5XGATwNYSBfmN5YEgeja9asWZo2GuJa3LiOchGRPmljSvqRjl2QLqTP//2//7c+s0OkPzpn0pJLQkrTSNXPuLKaX2bJzInSQX6f9mpo+rSf48r2Sy+9VF/7zdT4rvu/qGcvF1EEbMeduc5RBtIz1zn77MVS6ten0b5sKW3t/nLKbem7ffubScTYoulSoLZx3krny5bSNX+n3R4OESeDuOVEzCQrnfCI+8LELBz2l5NRzMQlCLMoLrrook1m0zDLeDmX120JcxM8YUCY3iCWRGW6cj4tT2VdDrKjQUobuGj0vVfE8pG+lNu0QyF9uZHYuHwh0kzjSpQ6D5xg0c/OEzzNB0KkH51sfoaCDnjctbDbbLNNfc1xOqDRdI0rq7Th3CgtvcySfCiV2y7t1dD0bT9LZZuzMLfddls90GI6K9PE+cs63DiRABWP/KwnaUoZt18cPgK2nHWL6cpnnnlm/RdFTjnllPp+ELRj6Yy4KEf5yST77MXU1K9Ps31pOrs/baV2/8///M8nLrdt6TBJf9OmNLbIsX2lcd5K58uW0LesTqs91Pwg78jDtB7Mg7kJntDYEa2jItEoMphMbyCrbug88ptoNSHyl0Y3487g++23X/1ak6O8MmUvjTQz7ZADnqZOnQMkOgwuMxnXqdI50Uktal2gvqfTM0k30q+pA86XpVH+2te+Vt/xnZvckfYc4EfaU8ZvuOGG1sGa2pXKKoEU8oCbv4J8YQpqqQ3v2l4NTZ/2s1S2STMuy4nprNzThMAJARTO1DAI5yQC64/vUrYp4/aLw8cZx8hbHvw5bv6iINfEc2DJgRR1JAaWXBcOykzOPnvxjOvXp9W+bAlt7f5yym3pu5P0N21KY4v0RAH4jdI4b6XzZUvpk7/TbA+Halyfnf5VphBp1KW8zavIu8hLUGeHfjJ4boInNERf/vKX64oU8qk+akdkmwY77zyowGlUl4r5mc98pi7YRIC5J0R6qYOWh06AxpIzyaQvnSZ3kydf0ryIDpWDpVg2HhyoggaVyG3pJlzzjnQhfUgL9p20IP1IR6Rn4PNlCbYyAKVjBt/heaQnZZxLTGJdmkyXsspfl6CspvkSbTjlPT+rN669GrpS+0kZpaxGmrSV7RJ+Z82aNfX6+35Xw0c7RR2hrpC/lJGmfgL22Yul1K9vqfZlUml/jFK7v5xy2/bd0u9Oom1skfdhpPG4cd5K5MtK6FNW25Taw3lBeuR9Nnke5TKtO6QRJ6LS8gbK9aIg7zi256oS9pEHdZb3hpyvq9Yzr62ndevWjZ5JkiRJkiTN1urVq0fPVoZ/bUeSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpYNX6DUbPJUmSJEmSlHHmiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBUYPJEkSZIkSSoweCJJkiRJklQwl8GTO++8s1q1alV19dVXj95RV6QZaRcP0nKcSGcexx57bPX000+PPtE0pHnRlr6lvHjxxRers88+e+nzSy65ZPTJYmGf2ffYz1L9z9Nk3LKxnG3JlhXpXmp/0Ke9Gpq0zh500EHVgw8+OPpkc13LdiyXpgPrZf3x3XlKI72MvD311FPHlpOmvM/1KXMaDvKN9pB2MbT1YWnbOK32pY8+62xbNt0XlmP5JqRT/nmpzOdpOI2xUVu+5NJ9y/N4FvkyRJO2S6RHnmbkYawr/2xesP9d++x8WR6LNsbP61SpDRiKuQueUGje9ra3jV6pDyroV77yleqBBx6o1q9fX/9/8cUXNzZkLPuxj31sadkzzjij+uxnPzuXDdUQkb5r1qypnnrqqdb0bcuLSy+9tNp9993rz9atW1f99re/XbhOmH1ln9l39pN0I/2aOh2WPe+885bSpLTs97///epb3/rW6JW2hMiftnQnv7q2V0PDNrKtse1f//rXq3PPPbdxQNCnbH/jG9+obrnlltGrjb9z8sknVxdeeGH9Xer/uO9quCgXp59+evXrX/969M7m8rzP9SlzGg7qaj6mbevD6N95HeOHabUvXfVZZ5Ttww47rF6Wx8MPP7w0RuH/tJ2//PLLq7/5m7/ZbF94nR80tpX5aY+N2vIll+cT32ObMIt8GaJJ2yW+R7lIkf+kT6QnaUd+kJbzom+f/Ytf/GJp2XhcdNFFo08XQ9QJ0iLylbox5Hydm+AJiUhk6nOf+1xd8NQP6UcFpRLut99+9Xv8z2AsXqd++tOfVieeeOLSZ29+85urRx55pHrsscfq15pc5AUNxM4771y/R6T1T3/6U2P6lvKChviOO+6oTjrppPqzbbfdtl7vz372s7nqUNrce++91aOPPlqnE0g39pO0yZEupE+kCcsycMuXJe1Ip1NOOWX0jmaNweTq1avr5+985zvr/5v0ba+GZu3atfUBUWzrwQcfXO299971ADLXtWwzuGLwn6YbA6tDDjmket/73le/jvpP2i1S/V9kHBDssssu1QEHHFDtuuuuo3c31ZT3uT5lTiuPg0fqPCdGaOdSpT6Mek2/lY4fptG+9NFnnbFNH/3oR+v/wX7ddNNNdZlmX/J2/i/+4i+qH//4x/XrQPCQNEiVyvwsxkZdxxZoyqcjjzyyrsvk/SzyZYgmaZdIuyuvvLI65phjRu9srC+kHYGDSE/WBdJyXvTtswk0kl6LKuopQTDSAgRbv/nNby69HqK5mnmy/fbb1xXunHPOGb2jrqLR58C7CxqoNLpJhd9nn32qPffcc/SOJkV0lU4zbRA5oNxhhx3qdM6V8oIIPNPcGHwHnkdwZVFEesWBN3gdA5EUnXR6kB2dLgOXEJ0zAVnSXVvGa17zmrrMfuELXxi906xvezUklC0OCl7/+teP3tk4QOKsY9PAuEvZ5n8GV+eff379OsV680EG66Sd0fARNCGvzjrrrNE7myrlfehb5jQMn/zkJ6v77ruvOvDAA0fvbFTqw8hXDiw4wAgxpmjSp+/squ8682XxzDPP1H0B+DzHQWNgvcj78FKZn8XYqMvYIjTlE9tFsIWD/1nky9BM2i4xI5g6kdeLHXfccZP8DOPK/lB17bNJv+eff76euROXtCzaJTvzemw5N8ETCtr//J//c6nRUn877bRT3aHE9XNdrj2kIWdZps+lkUEtT94JRIdSMi4v8g6Y9ZLXiybvcNhP0nEcOh6CIyzHgCU9a8WZiu222872ZAs7+uijl84atZmkvRqS/IAgHUDm2so2Z2EPP/zwzdKO4NL111+/dHBBmedAW/PjhBNOKPar4/K+SZ8yp5VFftIelpT6sBRlhLyPM/G5vn1nF13Xuf/++9cHhhwQh7Vr11ZPPvlktc0229RlmwPCCBjQxtOmBdLgxhtvXJrtkSuV+VmNjbrmS2DGJX0Y7XQ6A2cW+TJEfdolygEzdmJGTqC+MCPpsssuq9MfjOPm7bLrPn02wZT777+/+sAHPrB0iQ/BqEULoFAPaB+oIzyoW5HHQzWXN4zVZLiemkp611131RWRaw+5BKp0QEKnwLK33XZbfZ1iVHhteeZFPwxKOPNDmtFZR4NM50w9SAcxGp5J2qtFxP7+8pe/XJrmmyL4R7owLZpBx6GHHlodd9xxCzkA3xqV8l6Lb1wflop7hgz15BYHvdzHhG2MgyNmW73hDW+oP4/7oRA44DMOjtO+OWYhDOlER5d8SbGPLEs6nHbaaVtdH9YHl2ellzqlLrjggvp/AmKUFWavMCtjnvTps0kDAqMxe4lyRz3nMpdFKkPcjgPUER4EU9jPIQdQDJ5sReismC4fHSxnKbj2rulSkRyVmA7OKcDT8eyzzy5NWQWNBBHlLvK8yKf7sV6mxC4a0idtTNlP0rGL9D4xpc5Zw7Gc9moI8qnE6TT03Liy/dJLL9WXl3FJx7gDowiq8uASAA5CuBQtnyav+UJ5aMv7XJ8yp/mS9mEhAifXXXddMbiwnL5znD7rZNtom6KdYjYKaKvAZcnxGUEJZqUQlOgSPCyV+S0xNmrKl3G4NIFLFKIPm0W+DFHXdilOCNKnNaEdjKAVD4IppGE+s2XoltNns+yizSwnAJZe3sYss+VcXrclGDzZSkQnRePchsacSDods6aPBpLGPu1Q6OC5YSwdcaotL8hXGuA0X3m+aPenifRKB0K8phPKgyB0wEz5bLpu+I9//GP9eUT9yQumfb7//e9fuKmQ86xPezU0DPA4c5IOEKnHDPKaro0vle1///d/r/7+7/++PtigvJIuXHNP+R3XJhBUzaeDa/4wcOya933LnIat1IcF+itm5jETtRQ46dN3drXcdRI8GHewyD6z//wGyxEcYjnqAGWfOnDUUUfV9aNU5mcxNuqSL4FlWJbvNJlFvgxN33aJvotZCOQ1D8ZljM/e+973NqY5ZYCD7BgvzKtxfXZTeYvyPO/7HKgHlAfKxTwxeLKVoDE+/vjjN7te8O67797sgJ0KHHd/jkpLJeY6vXHXnaq7pvRlah6DibxTb8sLBk0MKNauXVt/Rt6yLNcR5w3xPIs7tJNOIC3Yz6YOOF+WNKHck07/5b/8l/r9iPozcPnIRz5Sffe7393kprxaWX3aqyGibqZTa9l2BsZxxjVVKtv5GVsGTvzFFdbNmRrWz+v4HdvpxdGW97k+ZU7DVurDKBcEz6jr/N92oN2n7+yqzzr5LA0iUD4JiDCuYYzCPqSXvTAzlHXzG3G5Szwo39SBCBiVyvwsxkZt+ZIiX2LsFvvGJUhsH+kxi3wZoj7tUjoDiQfjMsZnP/jBD+r0JGAYJ7lKaT9kpEPXPjvSKC9vjI3a6v28oB4gvyfS4E8Abyigc2dDRVzPpm+oWKN31BVpRtrxOPDAA9c/8MAD9fsbBmbrN1ToOm3DuGU1HWn6kvbkAfrmxbp169Zv6GCWPr/44otHnyyWSJfYz7T+85w0IC2QLzsuTSLt0nVp9prKOM/TeoBSuR+66KfybW8qc6WynRqXbvE9Hulnmh+Uj2OOOWZsGc/zvq0s2GfPj7z/Qt4mRB+Wv58+WM9y2pc+SuvM94dySHkc9/vsW3zGOll3E8p3/nmpzEdaxOfTGBvl+52us2n7SvuWr2sa+TJE4/Koqaym8nKU52f62TxJ04MHr0O+z6XytijmMV9X8c+GjZUkSZIkSVIDL9uRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKli1foPR887WrVs3eiZJkiRJkjRbq1evHj1bGc48kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBXMXfDkS1/6UvXKV76yfpxwwgnVM888M/pEXUySfnfddVf18Y9/vHrxxRdH72garrvuus55kS57yCGHVA899NDok6r+Ht+Pz8njRZTvJ2nSBeU3T1/ei/Xk6anZoQ2hLYm0byv3aXs1b21QnzJWKtt5mvFI023SeqFhyPOPcpPq00/Yri2OcfW6qT2Ix7i+fxZtRJ91ti1bKuOUYcpyfM6jaT8p+3kfkafVNMZG+Trb0jLtw/J9y9Ml3/5FMWm7RNqOS5NIu7y9HBr2IS93bfWhSZS7oe/vJPI6ldeTIZqr4AkF8JJLLhm9qqqf/OQn1ec///mFbGxmgQpKxXv88cerF154oTrttNNa04/l3/GOd4xeaVpI12uuuaZTXpBv6bKXXXZZdcEFF9SNC8vzvUMPPbT+7Kmnnqp+97vfdWqM50nsJ+nEfpIWpElbR0Ia5R0XHfcXv/jF6r777tssPTVbl19+ef0/5bSt3JNvaXt16qmnVp/61KeK7dVQ9CljbWX7iSeeqLbbbrulNONx4403VjvttNPE9ULDQHn40Ic+tJR/lBfKTRxc9OknbNcWR5SL6Nd5PPLII3W/vu2221Zf/epXl97nwVj4TW96U3XiiSeO1vCyWbQRfdZZ2hfwP2U1yu2ll15anX/++Uvl9t57763OO++8pe/y+PSnP11/FvjdpnEq/c1uu+1Wf2caYyP2mz4o1tmWlvRh/GZTfxdpmKYL640+clFM2i7xPZYd54orrqjL/ZBR1s4888zRq40mqY9R7q688srRO4ulz7hwKOYmeEJFiwaSwsaD5/fcc089uFQ7OiwaagbdOPjgg6vnnnuusYCS3kT/6LQ+8YlPjN7VNJDe3/ve9+oGIvLi6KOPrvMiL8sse+edd26y7EEHHVTttdde1YMPPlh//pvf/KY64ogj6s8YWB122GF1Xi8SOl72k3QCaUGa3H777fXrcehgKfOp66+/vnrrW99a7bvvvvXrND01OwyGfv7zn1fnnHNOXU5x8skn1wcC8TpEe88gOS33oCwMXZ8y1la2GVAQPMnTCJPWCw1DlIfIvz322KPae++96wPGPv0EbNcWR+TZhz/84fp/EBj50Y9+tNlBJ685SGesFnmfmkUb0WedpX0hsMD4Jt12/t9///2rW2+9tX7NWGbPPfesn+dK49TobyKgRPtJAJ7faxrzdkG9e/TRR5fWyX4zvmja7+jD+M1ou9P6G2mYpgv9XR4YmneTtEvkz1VXXVUdddRRo3c2RbpSdoZ6YpeyxwwbgiJnnXXW6N2N+tZHAjC77LJL/XwRT2T3GRcOydwET55++unqySefXDr4J1GpgLvuumu18847j5ZSyT777FM3OtH5MkDbfvvtxxbQc889t7r77rurAw44YPSOpiECHumAgDwgL8iTFO/TiNCYhPg+oh5Ew8tnDA7I60Xy2GOP1fuZllXSLy3POT5DBJZA+tDppunDOjnjs5zBpNpRtjkw5ACxix133HFp0JCiLAxZ3zLWVrb5/Ne//nXjlNZJ6oWGg/FMzCJCHJxxYqNPP2G7tnjyeg3qNGPhFEEGln33u989emdTs2gj+q5z3L7EsmkZDwRNKNfPP/98PXMh2r98Jum4cSpB51WrVm1yfEB/Qv1qCj52QQDghz/84VIggO1nn9MxRqBOU7fTkzfkXewz9TKOZxbVpO0SaUx+Nh17kH4ElTnYHjJO3JH/7Guqb9159atfXU8WYCbGIuo7LhyKuQme0FjRQEZUlugdU5gWvfGZJg7ASb/Xve51dSdE5zQuukeaHnnkkaNXmrb8wDA6lC5isEQEn++RhyBPWSdnOtJgy6IgfdKyyr6Sjk3otG+66abGaczIB2uLFmwaKvKQgVEMhMddz0z7w9nHr33ta0ufR5s/L/qUsVLZpp3eYYcdNpnSyjT4GGj1qRcaJvKSoBht+umnn750cNa3n7BdWwz77bdfHTijrQycwf/DH/4werUR5Yaz2+nshiazaCO6rrO0L694xSvqmbIERKI940z0DTfcUD+n7f+nf/qnuk7Q9tEGcjAeAZS2cWp+kEogZRrHC2wXfRdjaY5B0gBJST4Lg/rJvkR/mAeGFkWfdolywAnAmJmRY/xLmRnySXPyt3TSuU99pHwv+jEu6dFlXDgkc3fDWBChY/oS06GIOqsbGmYitjEIpwGbhxvz6GVxfXBMcYtBN3kZgwvyeFE74a7izEUMUjQMcc8qyioPOs1x9zGJtp2BBR0qZ6ouuuii+r2tCQHvNMjNGWYOCmJau+Yfg2POUlInCJZt7e331o7ywL0/6OvjgOKNb3xjHVBOxaUPBCiGqm1fONFD8CFO6hEwj0tZol7EySDaQMY+TPMnyLJS2A7a5BhHdznYo05z7JJepsP9MJi1EmO3NDC0tWLGBsHApoABeX7//fePnWWl+dRnXDgUcxc8SQMnX/7ylzeJ3mk8DrJJu/QMhYPwlfPss8/WnWWgkaDjLInAyZo1a5aCAjF4iij9UAYXs0D6pI0p6Uc65rp0sEydTHHAotkj+JHOimJm0Lhp1OkAlQfBFMpAfhZrqPqUsa5lG6QLg4vQ57saPg6m6KvJw779hO3a4qCPZ7Z1tH8RIEnPuHe99GMWbUSfdbbtCwHi+Iw2n1kp42Yn9Jk9woyXdBvTy2amhUvs2i4FisDJd77znU22/dvf/vbSrBXadcbnLLdoJzS7tkvsO5pm8pCP3Aflgx/84NIxzLyyz95Un3HhUMxV8ISKZeBE845yS9Aq7VBoSLmRGB1xEzpfpufefPPNW+VsCg6Y84EQ6dc0cOQayr/7u79bmrFAm8Fd2d/1rnfVjTEHnmnnzTrpzJquW9b0kIf5oKEP8o4OlXwdsghudC1jpbINZpbFoBKxLr7Xp15oeAiIjztr3aef6FvmNH/o19J71MUJsbb8nUUbsdx15vuSiv3iN/g/nx0d93xpu2yDfmL9+vWb3COGg9Tl3F+haXtKSB/qN/Xw2muv3SRtCA4tenCzb7tEMJBZCDFDiZk5XKp7yimn1Jdv3XbbbfWljXzGTCXGdYzvaEfnhX32pkgPykOaHvNgboInNFYxnY3KFAdGff5m+NaMSknl5JKOKKRc2kAlHndtoWaDDoUzDARDohNm9g+DiaZOnY6BTjs/a4E4gxOzh8hbpr2m19UugrhDe+wn6Ub6NXXARLDjLBaP6GAj8ERUO52ZE3c/H/LU50VAHiK/9n3cYJb2Ptr8eSvXfcpYqWyPa7fB9/rUCw0PfS/lgvKBGOdwXxsOOvr0E7Zri4P8ToOm5CmzTtOZwwQFCA60BZNn0Ub0WWfbvuQBRC7bYN38xrjxzXHHHdd6oEk/QX9BHwO+SzvK/TKagjZd5Pvd1i/Fn2BtOtlL3SdNIl1i+xbtILpPu5TOQOLBzBxOlhN4+su//MtNZi/FX1xlfJfOWhg6++xNkR7oOi4cirkJnnB5wtD/pvfQ0TAxIIvAEx0Y16LSUOcdnGaLDpIBclznS+PJ3bTpYNO8iIaVsh/LxoNBB3lHUIVleC8GUot2LyDShfSJ/SQtSD/SEaUzuDkGOZ/5zGfqRpt18WcOox5odshDBpHcDC7KMGccYmBJeU/P6sVlOvNYrktljDJKWY2zZW1lO2+307ai7bsaNsoD5YLykeZfHAx07Sdgu7Y48nJBnvI8rdfMosj/mgz6ti+TaFtn2h+37QtlPW3fKM9RxvluPr5h2a4Hy3kf0ue7TfL9jnXSRiPtwwgWcOPb9GRvPFgu9o1gadO6FkWfvnBr0KfubA1Ij9K4cKhWrSd03dO6detGzyRJkiRJkmZr9erVo2crYy7/2o4kSZIkSdKWYvBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVLBq/Qaj55IkSZIkSco480SSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqmKvgydNPP10de+yx1apVq+rH1VdfPfpEbfK0Sx/j0pH3Y5mDDjqoevDBB0efaBrS9CVvyKNxSnnx4osvVmefffbS54taL/rU/7Y0KaWnZifPw0suuWT0yebyPOTRVk+G5M4771za7rYyVirbbenAell/03c1X8g78po8z8tE+hiXx33KnIaDfIt8D21tZZrXPEr1vtS+TKrPOtuW5XV8xnIsH/L2jUeaFqV0yNvOUn/TVb7OrmnJcvnv5/vGviyiSdsl0iyvF6WyMi/65Pss6u7Q5HVqHvJ1boInJO5nP/vZ6pZbbhm9U1Xvf//7F7IgzcLOO+9c/fjHP67Wr1+/9Pjud79bvfOd76wLao50XbNmTfXUU0/Vy37961+vzj333LlsqIaIxjJN3zPOOKMu32knEUp5wfLnnXdetfvuu9efsQzLLlonHPWfdGrbz7Y0sWyvjMjDww47rE73devWVb/97W/HtuGPPfZYtf3229fLsTwP2jDasqFjcHTxxRdXDzzwQGsZayvbpXRgfaz3wgsvbPyu5gdl5itf+croVf8+u0+Z03BQV9/2treNXm3U1laS1x/72MeqO+64o/68rT/s2nd21WedlL/TTz99aV94PPzww0v7wv+U+yi3l19+efU3f/M3S+X2F7/4xVL7Fo+LLrqo/qwtHS699NKlcUBbf9MF+z3JeIvf5HglxbaffPLJS/vG9i03X4Zo0naJ76XtIUgb0oh0Z12lcfNQ9cn3trqzKKiniDHOPOTr3ARPGEDefffd1Uc+8pE6gWksQUFSf9Ew0QnlByMU2J/97Gd1AY7PDj744GrvvfeuG0AtD+lLY5mmL4PhP/3pT3U5T7XlBcs/8sgj1UknnVR/xjI0tD/96U/r14vi3nvvrR599NGlgwb2kzRp2s9Smli2Vw7tNnl45JFH1q+33Xbb6vDDDx/bhjNA2m677erl5s3atWvrA6L99tuvfl0qY21lu5QOMYh885vfXL+Oss76ND9ol6688srqmGOOGb2zuVKfjT5lTiuPAyPqPAf/HEil2tpK8vrEE0+s6zooDwTa4nWqT9/ZVZ91Rvn76Ec/Wv8P+uabbrqpDmbQH7P/UW75/y/+4i/q/QH7TDluUkoH6gvHCTEOIA3ZRn6P+jaJvuMttoEZBoz3OHZJERQ65JBDqve9733169g+lp10+4ZoknapqT3kva7j5iHrk++lutMWfJoXUU8JSsYYh4DRN7/5zaXXQzQ3wRMq3n333bdZgkbnon7yTidF+pLOFOAQnbmWL9IyHRCsXr262mGHHeqGNdWWF9QLZmNFx0SDSgR70epFpBfpFHjNvuadSClNLNsrh7wjz2KgyUCBgezrX//6+nWOPPnnf/7nuZrKCfaLg4J0vyh3nK1sGmS3le1SOuyyyy71e9FuRFnn+5of3//+96sDDzywfoyzttBn9y1zGoZPfvKT9bg2z/dSW5n2Z1306Tu76rvOfFk888wzdfAXfJ4jaMJ+P//88/XMhWj/4tKXtnRg3SxPGxl4TvBj0oPtScZbV111VR3QoS7meI96miJtGZMsgknbpab2sM+4eej65Htb3Zl35N0+++xT7bnnnqN35sNc3jCWxpNIJtGqpoGEyvKIfBc0/lRiosZavh133HGTTj06lC6a8oJOimsGWSd1YhHrRd7hsK+k4zhd08SyvWWQdwSuwKCWAQFnXNJAVorB86te9ap6QBFTOVl20gH/lpYO8jAuSIRS2S6lA2fgfvCDH9QHVqTpUUcdVU97t1+cH+Qj+Rdn8Zt07bP7lDmtLOru0UcfPXq1qba2MtoGygyf8+Agfpy+fWcXXde5//771weGHBCHtWvXVk8++WS1zTbb1DNqGNNHu05Zv/766+vntHn3339/9YEPfKBu+3jNwXgEUNrSgfqQHniyjTvttNPo1eS6ji0IsjDzpAmzBdnP2F7WyeyDRdSnXSq1h+Q3aR4of13HzUPRJ99LdWeRkIfsY9Th/D43QzSXwROmrRJ1owGdh0Qemr6RvrgmNZ1WpZUxLi94zmCLAQYdk/WiW5pYtrccBkUMiMiLGAgzaIiBcI52nvyLfGGaK4Mwgl1bk1I6cKBx6KGH1gdWpOltt91WX0++aNdEL7JvfOMbdf7FVPQm83p2TpNpayufffbZ+jn1nM+Z3s/lP6UAykqhXBPQpZ+Ng6MDDjigesMb3lB/TkCI4AMHxXx22WWXLV2mwHdp5yJoRBtIX00g8V//9V9XLB26jC3aEFjh/h+cCGa/acePO+64ZQe15l2X9nCe9cn3trqzKD73uc/V/1OfeBBMoZ4P+RhmboInNI4UnBgUEk1mAMl9UObpereVRmEkqku0v8vBIulNxb3uuuuWpipq+ej002l35AtnVEq65gWR7eVMTR0q0idtTEk/0rGLpjSxbG9Zcf1unFFKB8IEAdqw/DydZeKMUSruV9CkT9lO04GDaq6fjllTDLYItnCgFWdyNVxxkFeaKdSnz+5T5jRcpbaSoAEHWhxUxQEm/ReXdI27FGI5fec4fdbJ9nF5UhwccUYdMYuANis+IyjBmXWCEk3S2SNt6ZBfCsE2csnDNC1nvEW9j/0mfdg3LkPJL9OYd13bpbb2cJJx8xD1yfe2urMIuCwvAqRghuXQj2HmJnhCA8X1bzEoJFEJnLz2ta9dqEI0a6QbhZL0bENEn/TmbKYHl9MTgb+0Q6GD58ZX4/JlXF7Q2TDAWvQDpUivdCDEazqh/AxFlzSxbA9bnHlNzyDGQImyMGQR3EgHiLHtTdfGl8o25jUd1I6DPM66xVlF/iLHt771req9733vUvvVpc/uW+Y0v7jUhQOt9CCypE/f2dVy10nQd9zBIuWe9o7faOrLY7/32GOPYjpwXMDBZvo5z5czg6vL2GI5aA+ox21B0nnRt10qtYcvvfTSUrkLbePmedEn30t1Zx6Rp5QHysU8mZvgCY0dZ9i4WRONItG3X/7yl8vqALZGdB40Sm0BJ87K01Hwv+k7XTSQTEtMzw4zNZUGsalTL+VF3Lk8LmWgAWLaa3p380WQ7yfpRvo1dcBtaWLZXhlxxqRLWSVfaNvJ4+hU47rfebg3DWdO0hk18dcpIg1SpbLdlg4MGjmJwPrBdwkMLvK050WSnnHnwZ8i5q9ycB+byL+ufXafMqdhK7WVnESkflPPY/wQ9wnp0h+W+s6u+qyTz9IAMNvKjE/2gbEQ/XB62QuXbbBufmNcOhx//PH1wWYpHehTSK+1a9fWn/FdtrHrrOsm+X6n+dJ3vMW28mfHo76SPmx7n3sRzoM+7VKpPYz8Jg8jv8mHcePmoeqT7211ZxFQp5Df12Xwl6luKKBzY926des3VKT1bDaPiy++ePSJutrQGNVpSFqmNgzQ1m+o0Os3NHJLzyOd0wff13SQlpGupDfpjr55kS+zqPUi38+0LOblelyadElPzU6e/mmeUd75jGUC+RbL5p8NHfsT277hgGf9Aw88UL8f/Vha3vJ0yctiKR1YL+uPz+0X51fejqHpPUSZoZyFcWVOw9aUx3mbkH+e5jWPKAeTtC+TKK0z35+8jcp/v9S+5b+Tt2/j0gGRFuO+O4nS9vDb+fYHluuz7Ysk3c+2vjCVlyPwXqxrXFoPXSnf+9adRZDX0zzPh2gV/2zYWEmSJEmSJDWYy7+2I0mSJEmStKUYPJEkSZIkSSoweCJJkiRJklRg8ESSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVrFq/weh5Z+vWrRs9kyRJkiRJmq3Vq1ePnq0MZ55IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCuYyePLiiy9WH//4x6tXvvKV1V133TV6VyXPPPNMdcIJJ9Rplj+uu+660VKb4v1Y5pBDDqkeeuih0SeahjR9yRvyaJxSXqT1gce4/Jx3eRku7WdbmpTSU7OT5+GXvvSl0Seby/OQR1s9GRL6ptjutjJWKttt6cB6WX/TdzVfyDvymjzPy0T6GJfHfcqchoN8i3wPbW1lmtc8SvW+1L5Mqs8625bldXzGciwf8vaNR5oWpXTI285Sf9NVvs6uacly+e/n+8a+LKJJ2yXSLK8XpbIyL/rk+yzq7tDkdWoe8nUugyc//OEPqyuvvHL0Sl3stNNO1Y033li98MILS49vf/vb1Tve8Y7q6KOPHi31MiroNddcUz3++OP1spdddll1wQUXzGVDNUQ0lmn6nnbaadXnP//5TTqJUMoLlv/Upz5V7bbbbvVnLMOyi9YJs5+kD+nUtp9taWLZXhmRh4ceemid7k899VT1u9/9buxg4Iknnqi22267ejmW50EbRls2dAyOvvjFL1b33XdfaxlrK9uldGB9rPe8885r/K7mB2WGchL69tl9ypyGg7pKnqba2krymjr/k5/8pP68rT/s2nd21WedlL8PfehDS/vC45FHHlnaF/6nrEa5vfTSS6vzzz9/qdzee++9S+1bPD796U/Xn7Wlw+WXX740Dmjrb7pgvycZb/GbZ5555ujVRmz7GWecsbRvbN/3vve9ZeXLEE3aLvG9tD0EaUN6k+6sqzRuHqo++d5WdxYF9RQxxpmHfJ274ElThVJ/kY50QvnBCAX2zjvvrAtwfHbQQQdVe+21V/Xggw/WrzU50pfGMk1fBsPPPfdcfaCUassLln/00UerE088sf6MZWhob7/99vr1oqDj/c1vfrN00MB+kiZN+1lKE8v2yiHtycMjjjiifr3ttttWhx12WD0YaEJHStCA5ebN9ddfX731rW+t9t133/p1qYy1le1SOjz99NPV+vXrq4MPPrh+HWX9scceq19rPlA3rrrqquqoo44avbO5Up+NPmVOK48DI86wchD1iU98YvTuRm1tJXn9nve8p67roDwQaIvXqT59Z1d91hnl78Mf/nD9P+ibf/SjH9XBDPpj0iDKLf/vv//+1a233lq/Zp/33HPP+nmulA7Ul5///OdL4wDS8NRTT61/j/SdRN/xFtvADAMO+M8666zRuxsRFHrLW95Svfvd765fx/YxNpx0+4ZoknapqT3kva7j5iHrk++lutMWfJoXUU/POeecpTHOySefXH31q19dej1EcxU8obAQsaTg5Z2N+sk7nRQFloJLAQ5UajpLLV+kZTogIM233377umFNteUFHRIzsaJjoo4QwY5B16LgYJAON21MST/2Ne9ESmli2V45pD15GANN0p2B7D777FO/zpHnv/71r+dqKifYLw4K0v1i3zlb2TTIbivbpXTYeeedq1WrVi21G1HWxx1saJhorw444ID6MU6pz+5b5jQM5557bnX33Xdvlu+ltjLtz7ro03d21Xed+bJguVi2qb0iaMJ+P//88/XMhWj/4tKXtnQg6EzbSBsZdtlllzr4MenB9iTjrSuuuKIO6FAXc7yXpwtjEfZ7EUzaLjW1h6yr67h56Prk+7i6w4mTRUDe7b333tUee+wxemc+zFXwhEboySefrCNUnInTZPKIfBecBaASEzXW8u244451Rx6iQ+miKS9odLlm8HWve109uG4aYM+7vMMh/UjHcbqmiWV7yyDvCFyBQTD5xxmXNJCVYvC8ww47bDKVkymsMeAeuvyAYFyQCKWyXUoHzsBde+219YEVafqud72rnva+iPV/UZGP5F+cxW/Stc/uU+a0sqi7Rx555OjVptraymgb0nshcBA/Tt++s4uu69xvv/3qA0MOiAOBwD/84Q/VK17xinpGDQGRaNcp6zfccEP9nD78n/7pn6rTTz+9bvtoAzkYjwBKWzrkB54EUmLWwnJ0HVsQZBkXEGW2IPsZ28s6mX2wiPq0S6X2kPymnAXytuu4eSj65Hup7iwS8pB9jDqc3+dmiOYmeEJBu+SSSzaZ3qfJ9I30xTWp6bQqrYxxecFzBlsMMOiY5qHxmbUuaWLZ3nIYFDHIJS9iIMygYdxN/Lg8gfyLfGGaK4PhmM69tSilAwcab3/72+sDK9L05ptvrmdnLto10YuMk0LkX+mgbl7PzmkybW3ls88+Wz//zne+U3/OZTSMjUsBlJVCuSagSz8bB0dvfOMb60tzQECI4AOBCD772te+tnSZAt9l1kYEjWgD6asJJD788MMrlg5dxhZtOI4hTbjfDftNO37MMccsO6g177q0h/OsT7631Z1FwbE9qE88CKZwb6EhH8PMRfCEBIzIHDddogBFYlMAHSh2R1oS1SXa3+VgMQ4u16xZY9Bqiuj0GRAF8oUzKiVd84LI9nKmpg4V6ZM2pqQf6dhFU5pYtresuH43ziilA2GCAG1Yfp7OMjG1PRX3K2jSp2yn6cBBNZexxqwpBlsEW7jGPs7karjiIK80U6hPn92nzGm4Sm0lQQMOtDioigNM+i8u6Rp3KcRy+s5x+qyT7ePypDg44ow64pIa2qz4jKAEZ9YJSjRJZ4+0pUN+KQSXOky7XVzOeIt6H/tN+rBvXIbSVs/nTdd2qa09nGTcPER98r2t7iyCiy66aJMZyMywHPoxzNzdMFbLQ2GkUNLgtyGizyCcs5keXE4PDSRnjtMOhU6AG1+Ny5dxeUFnk94DYVEx7TMfCJF+dEIxcApd0sSyPWxx5jU9gxgDpXwK8NBQvwlupAPE2Pama+NLZRvzmg5qx0EeJ4LirCInh/hLgqeccspS+9Wlz+5b5jS/ttlmm/pAKz2ILOnTd3a13HUS9B13sEi5p73jN5r68rjXA+W9lA5c3sHNtNN7Q7DscmZwdRlbLAftAfvVlC7zqG+7VGoPX3rppd7j5nnRJ99LdWceUc8pD2lbMg/mInhCIYkpcvEgUgX+RNm4a+a1OTqP/CZaTTgrT0fBdMhJO1g1ozwzLTE9O8wUfBrEpk69lBdx5/K4lIEGiGmv6d3NF0G+n6Qb6dfUAbeliWV7ZcQZky5llXxhIM6Mw+hU47rfebg3DWdO0hk18dcpIg1SpbLdlg4MGu+55556/eC7BAbTv0ig4UrPuPPgTxHzVzm4j03kX9c+u0+Z07CV2kqm7TN+oJ7H+CHuE9KlPyz1nV31WSefpQFgtpUZn+wDYyH64/SyFy7bYN38xrh0OO644+qDzVI60KeQXtwjAnyXdrTrrOsm+X6n+dJ3vMW2cglm1FfSh23vcy/CedCnXSq1h5HfXcfNQ9Un39vqziKgTiG/r8vQL1N15slWhqgtjX9e8dJKynMaKAJTcR1qPOjotHwcEHGAE+lLevN3zcmXPnnB8nyPZXiPsy10MnRCiyTfT9KC9CMdkQ7ASmli2V45HAwSsErzBfzFCVDe07N65Bf5xnIsz/eijgwdA+nPfOYz9cCAbec6/JheThmlrEZ5ayvbpXTgd9asWVOvP77L9zyhsDi69NkolTnNl7a2kjpOuxB9GHnOQRXv921fJtG2zrQ/Zl8oh9FGsa08j2Vpq9L2jfIc7VtTOrBstG+ldADpxVntpu9OIt/vWCfbgLwPK8nrK7cgYNv7BmGGrk9f2IZ8HTdunhdt+d6n7iwC8u7LX/7y0k3veVBneW/I+bpqPfPaelq3bt3omSRJkiRJ0mytXr169GxlOPNEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqWDV+g1GzyVJkiRJkpRx5okkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVDDo4Mkll1xSrVq1qrrzzjurp59+ujr22GPrB88ffPDB6qCDDqrOPvvs6sUXXxx9Q5IkSZIkabqceaJaBKcIVIH/I1CFq6++2kDVQKR5wYPnvIc8H2XZnhelfMnL+daqrb7naajFZbumJltzG1HqM/J02NJsu/vL06SUv4tiyGV4WhZhH1et32D0XJIkSZIkSRlnnkiSJEmSJBUYPJEkSZIkSSoweCJJkiRJklRg8ESSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVrFq/weh5Z+vWrRs9kyRJkiRJmq3Vq1ePnq0MZ55IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCmYaPLnrrruqV77yldXHP/7x6sUXX6zfe+ihh6pDDjmkfj//DF/60peWPjvhhBOqZ555ZvRJVT/nvfj8uuuuG32irkizSD8e5NE46bLkGXmn6UnTNy/ruS55QT2iPpXydJ71qf/5srQrqT5pr+nrWlbTfOIxT2U7+j8ebe1nW9lO0yHvM/M+dVHr/6LK829cuxZlpJS/fcqchq1Pm7Dc9mVSfdbbZ39YjuVzpX6D9/K2McXn49bbR2zDuP3IjTumSfc3fUxjG4emT7uULhuPNI27lJOhYZvzMSjY9g9+8IO96m7TeuZdXqfmIV9nFjxhx/NM5r0LLrig+tWvfjV6p6quvPLK6vLLL6+fU8AuueSS+jl+8pOfVB/60Ifq75G4n//85+v3wplnnrlJpVIZjdJll11W3XfffdULL7xQ///FL36xseKSrtdcc031+OOP18vyPfJu6AV6XpAXafqedtppdflu6vi75AXf+9SnPlXXp0UU9Z90Ig1IC9KEdMyRLrQbsexTTz1V/e53v1tqK/g/rQeXXnppdf7551u2t5CuZbVPezU0bCPbGtteaj/bynZe/3fbbbelPpPfOeOMM6rzzjuv/oyy/r3vfa+xXmh4Iv8oH015n7riiis2Gf/k+pQ5DVv0YYceemidlzweeeSRTfqwtjFB6NN39jFJn1zan7Stb+qTS/0Gv/mOd7xj9GpzrGcaB52xDbTBXdKS32TsQbvM8qRVjPNOPvnkpbSIdbEPLLPTTjuN1jD/+rZLt99+e93OpWlDWoF0Tst9mp5DRdnmWDUXdeKBBx4YvbO5WCbqWD6WXRQxnmmqJ0M1s+BJU0f/9NNPV08++WR11lln1YlEZXrTm95UVwgKxJ133lm/jkr27W9/u17Hgw8+WD3xxBPVPffcs/TdWDcNsNpRCBlUM8jed9996/f4/4c//OHS68Cy5EXaiB900EHVXnvtVeeFlifyIk3fo48+unruuefqcp7qkhc0pLvsskv9vDSAmGe0Cb/5zW/qdAJpQZrQ0eYiXWLZbbfdtjr11FPrdHz22Wfr//N6sP/++1e33npr/Vqz07Ws9mmvhuj666+v3vrWty5ta6n9LJXtpvp/xBFH1H0mA6t77723estb3lK9+93vrj+Lsk7a8V0NW+Qf5QPj2rUYI5XqTJ8yp2GLPPvwhz9c/48TTzyx+tGPfrQ0Vi6NCVJ9+s4+JumTS/tT6pPH9Ru0gZyl5ruf+MQnRu9ujuMRAjfLxfjs0Ucfrbcd7DPrbdpnto16S3tMuwzSqmmcB7aRPIy2fFH0aZfos55//vmlvE7FmCAt96X0XGkEjZhlQ7CH49YUQbXXve511Rvf+MbqNa95zejdzUUaRR1Lx7KL0r+TTj//+c+rc845Z6meECz76le/uvR6iGYSPKGhYwbJ1772tToY0oaKVEqkxx57rK54d99992YJykBS7aLRP/jgg0fvjEf6ks4R7QUVlY5Syxdpueeee47e2Zjm22+/fT2YTnXJi1e/+tV1JJ5I7aKiDcjbCdIvDiJTDGZuvPHGpQ4WfD+Vpn0wEDt7Xctqn/ZqaKifHBDss88+o3c21mPOVjYNsktlm3Xl9Z91UMajfLPe9LugfeC7GramQWLeDtG+cdDA4HKcvmVOw9c0LqYscGKxz/isT9/ZR9/1jtufWLbUJ5f6jXPPPbc+NjjggANG72yK7cE0jhU4DkkD+Gw7629aN+0z45A0aMMJ5Ka0aTqAXAR92yWWJ3jCbLy4hCNmWUQZT8sJ62oaNw8FATHKAPubImjCJAAu2SnpMpadd+Td3nvvXe2xxx6jd+bD1IMnNAJMy7rooouqww8/fPTuRjQ4RIiZdkdkMc620GDsuOOOdQHjkh7ep9LEVKd8MEHUjugzs0/ShkllVEAqbFxf3eeaaM4A0PlFnml5KO9pdD06lC7yvDjyyCM3aVwXFemTDixIP9KxDYMVov9E7Fn+sMMOq9uQGMRQB2644Yb6uWarT1ldTns1BPnBQDqAzHUp2wwiSQcG63EGl+ASZTcOEBhgcqCt+RQHUXFmG7T3tFk777zz6J3x+pQ5Ddd+++1XHyhyoB44g/+HP/xh9GpTbeOzSfvONl3XW9qfV7ziFa198rh+g/f4bBzaw5tuummT+jQNrJd7NDB7gGOQrsch+SyMMO79RdG1XSK4xBUGHEMSJCRgxtgtAiiUrUnHzVsaeTkuoHf88cdvUm+6Sseyk3x/qMhD2gbGNzxK9y4aiqkGT9hZZpvsuuuum0zPS+WBEIIlDJDx6U9/ug66BKbhNc1cYTkqFY3tPCTyUHBtHQPrf/iHf6gbJhooIrxtByQ0XCy7aFHxeWRe9ENnE9daxwCHM3Y8Z+BDQ02bNa690sqZtL1aVJRb0oH7AcRN5higkS6cTKAsv/3tb6+OOeaYqRwYacsiPynfp59++tJBFO/df//9CzeVX2UEBajn1O04oOBsNZey5OZhTNC2P7Pqkzkg4wB22kEJ0pnZP7THBAK6HIdwvJIGvgNjlHGzV7Y25BOziGKsRrnheI+AAZdcb82axrKLIu51Sn3iQTCFewsN+dh+qsETroFkVgkzQmgEiYITHOG9U045pbrlllvqRCJAQgIRAGHQl0acqSiRgETn+D6NU5x1iwgkjReRdqKUQ7zebYjoqJj2GB0s+cP11qUpb9Exr1mzZuod0NaMjiCChqCRYHpjydaeF6RP2piSfqUONe1saFdSaTvDIIgzYJ6lHZZJ2qshyafXli4L61O2md7KNNdIB8p3lGUGnsxQYCrzUA+ktLkInDAzNy7HoDxcddVVdaCsa172KXMatjiQjLrN7A2kM5C6jgn69p1d9Vlv2/5Mu0/eUoFHZv9xiWnpOCQCJ9/5znc2m0ET97WI9FhEy2mX0tlMk4ybF0FpLLsIiAmklyEyU6ytTq20md0wtsm4hOAmskzXooEhQBJTkOOaOKZ80UAxC4UIJAWJdRE4YZZLl+msW7tII9K5K/KD9L755psNnExRBP7SDoVOgBtfjbvHw9aeF7QB+X0cSD86k3wwAgZO73rXu+qbi7V1NrQntDn51FKtnEnaq6GgfnPmJB0gxiCv6exiqWyDGyJGn9gF/WY+nV7DRd4SGOQgOB1AMsa57bbb6s8YF3FCihNTnHCKk0ihb5nT/CFYmgZFu44J+vadXS13vfn+pKbRJ7P+v/u7v6sPvqk/cak/4wLGB5Ngm/r8GVXShlkp1MNrr722MV3ye1gtkr7tEu1aPouHYAnlhPTpO25eBH3GsvOIOk55SPN8Hkw1eEIDEJFjHsxEIeDBnYZpOP7qr/6qfs3sk3QwwNlEzqZFZYopyCzHdxk88DnL5bNaFrXRmTbS6LjjjqunQ0YhJX8IQDU1PDRidBRNkXItDx0K1yxGIBBcs0wH0XTTJPPi5Tu0k04g3Ui/pg6Yz/hTeOlZ3FTeQcdd7vkNDUPf9mpoOHPCvStikB5/maLp7GKpbJMODJq4fCnSganorIs78LN+zqzG79BOcK+AaV/jr9kg32inGNcwlknlZ+pjpi7LNrVrfcqcho02IA2akqcE1+JeB33GBH36zj769slt+zPtPpk6kh6PUG+oP8s5AZXvM9tLHzXufiXxJ1i//OUvNwaJ+D4Hjos867VPu0TfTh/PMqDcECSM+9X1GTcvAvazNJZdBFHH8/shDf0mslt05gmNC9ML0/uYEByJhoXBQ3rPk/QzHjxP/+QTyy5iJG5WqHzcmCsi8VTImO6Zdm48p4GKQBXLxoNOTstHWeegKNKX9I5LFMyLzZEupA9pwb6TFqRfHHCkgy86VNKLG06n6RVnjKgHnA2JekA6p5eHaGWQD5FHKLVXQ8c2fuYzn6kHBrHtXPPPgQ5llLIa9betbOflleXioCn/HQ4OOCiZhzTSxkEiJ4HihFE88rOvTdJ+AqUyp/lCnpF35CF5SZ7ynDahbUzQt32ZVJ8+ubQ/mJc+Od9ntpftjuOQtA8jWEAgO/5ABsvHI+osaUMgYTkzbIauT1/IsvTxUU7yMsX/vI5yTz4MsZwsR1pv2sayi4C849ieP78c+0dAcVzAcShWrd9g9LyzdevWjZ5JkiRJkiTN1urVq0fPVsYWnXkiSZIkSZI0bwyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgpWrd9g9FySJEmSJEkZZ55IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBTMNntx5553VqlWrqrPPPrt68cUX6/cefPDB6qCDDqrfzz/DJZdcsvTZscceWz399NOjT17G8nyPZfgNdXf11VcvpW9b+kX+xYPvanrSvBhX1kOaF/myPOe9+Jw6tIjy/SyVx7SNaFrWsr0y+pbVPnVkaNIyRp9H3zdO17Idy0W7naZP+pi3tNqa5W0RD/I1LxP55036lDkNB/mWj4XzPiyv02ndn1b70kefdbYtm+5Lvp8p0qmUDvlneRpOY2zUNrYYh+Xy3x9X9xfNpO0SaZHXizS/ebDueZMfB5f2oa0dWAR5evAY/HHM+hl56qmn1r/zne9cz0985CMfWb9u3bpN3ksfF198cf2d7373u5t9xvJ8L5Uud8cdd4zeVRvS6sADD1z/wAMP1K/5/5hjjll6neI9lo30jbwzvaeDdEzLNmU66kkuz7d02cgX3gPv8Vm8XhT5fpXKYywb7Uq+rGV7ZeT50lZWyY+udWRo8rY135dUn7JN2pX6vfjuotX/RUaedm17yNdx5ahPmdNwkE/U6bxto1yk76XtH99JxwSlvO7TvnTVZ53xWbT74Hl8l//zcekpp5yy2b7EetL9bPsuvxO/m2/zJGIdsc7Ypra05DfJ4zQNwOvl5MM8IE8maZdYnryNMg++m+f3uGOYoYr9inIYZWpcOaCMpGnA99LXi4B9Wk69XAkzm3nyjW98o7rllltGrzbaUFmq3//+99WGjK82ZHy1oRBVGwpRHXX77W9/W/3sZz+rX/P+hm2rNiRmvQ5eByJUX/nKV0av1BXRyzVr1lQXXnhhtd9++9Xv8T/pG69Ta9eurU488cTqsMMOq1/vvPPO1Y9//OOl15pc5MUZZ5xRpyuIJv/pT3+qHnvssfp16qc//WmdF5FPb37zm6tHHnmkXjbqBt/HtttuW6+XupRG6+fdvffeWz366KNL+0m6sZ+kTY50IX1OOumk+jXLUm5jWcv2yqDNJw+PPPLI+jVl9fDDD68efvjh+nWqbx0ZGsrY2972tqU6e/DBB1d77733Jn1Z6Fq2o5/cMPAcvbM5+l1+533ve9/oHQ0Z5fz555+vdtlll9E748XY56KLLlqqE6k+ZU4rL2ZjfOxjH6vHZSnyesPBVHXeeefV7SROP/306pvf/Gb9mvbikEMOqfbcc8/6s/3337/acccd6zF2rk/f2VWfdUb5++hHP1r/D/rmm266aWncn49L/+Iv/qLuk1O0bWkfTd0pfTfSMMYB0xgbtY0tcnFGnb6M455Un7o/zyZpl0ibK6+8sjrmmGNG77w8Juh6DDNUv/jFL+q6G310lEv2LS+Xbe3AomAMSJmYJzMJnjCt6nOf+1z1rW99qw6GtCHRSgWBRhp0Nueee25d8PLORmXR6HPg3YZ0ZqAeBzmarjiITBuL1atXVzvssEPdsOYYLPMILLPPPvvUAyc6bgYK6WA66ssiifQinQKvKaf5FMa8Q03Ls2V75ZB35FkMNBkoMJB9/etfX79O9a0jQ8J+cVCQ7hf92+677944yO5StvmfwdX5559fv27SNNDSsFHOn3vuuerkk09emq48btr+2izom+pb5jQMn/zkJ6v77rtvs3Fy2sc3oX24++67lwLJHIg+++yzjQfiffrOrvquM18WzzzzzFKwh89zaVCd9aKp3x73XdZNfUrThOdx4mkSpbHFOFdddVU9RqMupvrU/Xk1abv0/e9/v64Tab3ocwwzdOx/3kdTpygTqbZ2YBFQRggiXnzxxUv1YB5uPTD14AkDOM6OkBBHHHHE6N2NaHAIehBUoSElWg4GezvttFNdoH75y1/W75OA73//++vPoxEl8szMFZbfbrvt6vfUHWlMhxLXlpWuPeQsBjizEAU6OjAtH+mbdurRoZSQ/uQD9WvcAVIcZBHJbvp8nuUdDukX5bQJjTLXirIcBxxx0GHZXhnkHWdMQJrTB1BOOZPSZJI6MiT5oL4pSBTayjaDb2bpNM04CBxcp2f4NHz0xxwEf/3rX69n2/Ka9js/iIrAWJzxHqdPmdPKoi4fffTRo1ebo03gIDL6qPTeD/Rl11133dKBN2XmBz/4wdj2oW/f2UXXdTKe58CQfQm0VU8++WS1zTbb1O0aB0sRdKGsX3/99fVzsM833njjZmWf3277bh60YRsZBy/XuLFFjraYcXaTrnV/EfRpl8hLTqrErKZUn2OYoSL4QxmNMSdliXwfp9QOLAICRvfff3/1gQ98oK4HvCbgNvQAylSDJ2ToZZddVr32ta/dZIpeKp+iTbCEygDOrhN0CQRaIvJIQWM2SzplS/38+te/rivpXXfdVRdSGm0636bGh7MYFF4acpblzAbTSz3IXDl00OTFbbfdVs/AyvOCTocD0VJnvjVhcMXBOmlGZ02n89JLL1m2VwjlkwEReUG600nSHg29k1xptM/0k6VLcUhbyq8zquYLYxlmHkR7zcEv4yDqRRwQYms4A6lNMd4FbSUPDqI4acI4m76LMQBjAT4jCH3ooYcO8kCSMn355ZfXJ33iAPCAAw6o3vCGN9Sfx5iFQASfcQyRHj/ELISmcX/bd2elaWzR94C2a93f2nCSnPLcFAjscwwzVOQ7282JDsos9fa4444bG8wstQOLgHzm5FCcRKNusX+cLBhyvk41eMJ1kMwqYVobjRkRZwZ9vPfe9763uvnmm+uCQICEQkDQhOu308gxjUcUkhNOOKH+/h577FFXGDAbhQIXBYoCuIiR2lmgs/rCF76wdLaAaw+5BKppGjwVmQ4vGjAqPFOGnQI8HRzAR9AQNIREW7sgT+hw07yg/sRAIr3EZ5GQPmmHQfqRjl3EfWKeeOIJy/YKiWuc44xSWye5nDoyBJxtTeUnDlLjyjbBPq79Puuss5ba7SaRtjGbU/OLsVM6kKZccCaWs+ylMoA+ZU7Dxjg5DijAzAv6sH/5l3+py0N6gEm/Tx+2du3a+nVuOX3nOH3WGYGCGNtHO0VZRzruJyjBrBSCEl0Cx+O+i/xSCLaRy4WmKb0H3XLldX9RdG2X4gRWBJRyfY5hhoz9izJLvSDfuSQ5v7QN49qBaZS3oSI9pjFDbJZmdsPYJo8//vjo2aa4FIdGjSAKgZGoQHEws9dee9X/a3LRSZHObajAVOQuy6o/0pdpjGmHQgfPzTDz6zkZnHBWoxQgZIBx1FFH1YOpRQ2cRHqlAyFe0wnFADLQfnCA3nT2hmnClu3h61NHhoaBHWeH0gEi9ZiDjXHX7I8r2//+7/9e/f3f//3Spay045ycyE8a0Fc21QUNG3mYn7WmbUoH0l2u9e9b5jRstAl5cGJSffrOrpa7Tg52xx0s0m/Th/MbLMeMFZaj/aPdo/1jvNMUcE+/S1vJwWna1/N8OTO4SmOLvrrU/XnXt12iH+PEOHnNg5PlcfL9P/yHjYerizh2Y79Jpzw4Ps12YKia6lTkcRy3DtFUgyc0nBFN48HZMKbbcZdprsf867/+6/p1VI4YCMZdw6MyxXQmluO7TGuKKXLxiMt7OGuZRuXUjA7t+OOPr6c1RkVkphDXXOaDMiowB+LpjCA6Kq7TcyC2fJG+6fRMpq3RaeadetOyNDbkBRFo3mP6LpezLXI9iDu0k05gv0mTpvKYL0t5p9zTrtD+WLZXRpxtbMoXzkym+tSRIaJupjNq4q9TNM0MKZXt/Iwtgwpma6b9XgxG42yr5gd9L30w5QPkPW0TZT8G0uR5jJdK+pQ5DRttAvL7hHDg/5//83+uZyGNGxPk+vSdXfVZJ59xcMQ2gvJJQCTKeB5EiL8Yxm/QxqXjfso37R+XK9E2lr7L5/QtpBtYhm3sMoNrnHy/S31Ymy51fxH0aZfSWUQ8+Iurcfy47777dj6GGTLSgTIc6dFWd9HUDizKJZzjxoXk9aBPBm0ooDPzQMPf6Y73+Gke6We4+OKLx36WiuU2VMrRO+qCv6Ud6Us+kB/YMEDb7O/V8zyWNa2nL80L0p48QFNejMu39P30ka5vUUS6xD6y74HnaXuRL0t7kbJsr4w8X9I8Iw/ycjuujsyDtIyldZb9Zb/T8lsq26lYLi2vTe9pfuRjojzv87YtNOX7uDKnYWvK42gnIj/zz/lOfMYjysFy2pc+SuvM96etjKfjftbJupuwj/nnpe/maZiPAyaR73e6zqbtCyyX/35buiyKce1SU1lN5eUIvNe0rnmSpgePqLvI9zkvw3l6LIJSnRqqVfyzYWMlSZIkSZLUYIve80SSJEmSJGneGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFaxav8HoeWfr1q0bPZMkSZIkSZqt1atXj56tDGeeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgpmGjy56667qle+8pXVxz/+8erFF1+s33vooYeqQw45pH4//wxf+tKXlj474YQTqmeeeWb0yebfbfq+yq677rpN0o88GifNCx6kPXmg6UjzIi/ruXTZUj6Qn23rmlfsE/sW6UCatKFtoI3Iy3mftNf0jcuXXKk/GLro/3i0tZ1dy3Ys15RufPbBD37QNnrO5Hm/nLaqT5nT8JB/TWPaUns57jupru1LH33W2bZsqYzn36VPSHUdp5JO+bon0bY9uXT78nxiO9ne+LwpfxcB+xX72KddolzkaZaWlXlNs0nzfVpleGiifYv0mId9nFnwhB3PGxXeu+CCC6pf/epXo3eq6sorr6wuv/zy+jmV4pJLLqmf4yc/+Un1oQ99aCkR77333k2+q36oeJdddll13333VS+88EL9/xe/+MXGhow0/93vflc9/vjj9bI87r777mrfffcdLaHlIC+uueaapfQ97bTTqs9//vONAyDqRboseUg9yhuXpjq3KEgX0od0Ig1IC9Kk1OnwnU996lN1G5MiPdN6cOmll1bnn3/+Zump2RiXLznKMm3QU0891VpHhoY2lbY1yti4Oos+ZfuKK66o+8Uc66WvfOCBB0bvaB5EvkXe531yn36iT5nT8JDX73jHO0avXlZqL8d9J9WnfemqzzqjjB966KH1sjweeeSRuh9GqT/O6wd9AX1CfJfPu4xTWW4aY6O27cnxm6RJbN+pp55a5yXpR30944wzqvPOO29pXd/73veWlS9DNGm7xPdYNkXapGUlby/nwaT5Pq0yPEQRA5insd7MgidNg7ynn366evLJJ6uzzjqrTiQK/pve9Ka60NAA3XnnnfXrqBjf/va363U8+OCD9fdpcNPPeXz1q1+ttt122/pzjUchpIJSYaNj4f8f/vCHjQER8mq77bYzbWcg8oIGYqeddqrfO/roo6vnnnuueuKJJ+rXgWWpF+myBx10ULXXXnst1YtAnWOAsoio87/5zW/qdAJpQZrcfvvt9escg5lddtmlfp4OLiM983qw//77V7feemv9WrMzLl9yDBToFxhsRhs0ro4M0fXXX1+99a1vXSpj4+osupbt6CfzdGNA9brXva564xvfWL3mNa8Zvat5EOUh8n6PPfao9t577/pEUZ9+An3KnIaDto4zrfRJn/jEJ0bvbjSuvSx9J9e37+yizzqj/H34wx+u/8eJJ55Y/ehHP1oa94/rj/P6QV9An8B3qB9dx6nTGhu1bU8q+rBPf/rTS/WXOgnSjzr+lre8pXr3u99dvxfros7n65pnk7RL7P9VV11VHXXUUaN3+h/DDNWk+b6o43uCST//+c+rc845Z6ken3zyyYM/tp9J8IQGnxkkX/va1+pgRxsqUimRHnvssbpQ0dAy84TKx9SetqmKehmDrUcffbQ6+OCDR++UEdziO3TcpLVTgKeHMsvAY8899xy9s7EB3X777euGNcX7NCI0JiG+n4qo9RFHHFH/v2hoA/J2gvRjvxmk5F796lfXZ3uIXjdJ0z4QnNVsteVLYLB54403bjJYYKDclNdDE33VPvvsM3pnYz3ebbfdGg8uupRt/mdwxQAjR9CE9ppLdjRfKN+U8zi4SvvpPv1E3zKnYTn33HPrGRMHHHDA6J2NSu3luO/k+vadXfRdZ9MYn+Vi2XH9cV4/wG+HLuPUaY6N2rYnt+OOOy4Fv1LxHepnni7UeerzIpi0XSIgQrlOy3bfY5gh65vv0yzDQ0NfxgkDThzMk6kHT2i4mFZ10UUXVYcffvjo3Y2IDhI1ZPohDUpEYRkQ0shQoNLgyJlnnll/TiMaA4kU64kpcGpHg09nE9falQIiNO5//OMfl6YbkqdMNTOAMh15pxodSheckWEwEvWH8n/TTTfVZ3MWWd7hkH6kY5MjjzxykwFO4PuHHXZYfbY+Bm6U6RtuuKF+rtkaly9d5Gewhi4/IEgHkLm2sk2dp9zuvPPOo3dedvzxx282ENN8oS1iJgFt+umnn75Uxvv2E33KnIaB9pB2scm49rL0nSZ9+s6uuq5zv/32q8fuHBAH2vI//OEP1Ste8Ype/THLcHlQzEhsG6fOemyUb0+KPGIGDSeR4xiFGSdx+RVBAPYzDoxZhgD5IurTLpGmzOSJ2T0p0rTrMcxQ9c33rWF8T1tC+0Ce8piHiRFTDZ6wszQUu+666yZT9FL52V2CJVQGML2NoEtgOmLMXMkv+eHB83vuuaeOSKod18NTSf/hH/6hNSDCTIc0wk7E/T3veU/d6WnlMKuLfEunuEWUfp6mLq4kyjblmUsdaKhps8a1VxqGuHZ8a8wn2uf7779/aZqvFg/9LP0t/TJjpEW9tl1bH8o29zFh3BIHR8yWI7CArv0xB9Vx7xQeaBunznJs1LQ9OWYHgcAS+8ZsizjGYZtIEy7H4rO3v/3t1THHHLPsoNa84/IUglGRp6k+xzBD1Tfft4bxfdzrlDzlQTBl6BMjpho8iagq9ymhIeQsCsER3jvllFOqW265pU4kGg8SiGgxBSiNOhNAiQTkjBrfJ0pJwWGKYlwHxYMEToMvKqOzYvpnHHSTP1x7l08BHsezWNPz7LPPblJuaSSY3lgSgZM1a9YsNaRb04EV6ZM2pqQf6TiJtJ2hTeEsmOV7mCJw8p3vfKdxQDVU+XTu0mVh48r2Sy+9VF/7zSU5+ZlNLSamZlPeyf++/USfMqetxzT7ztBnnTF+jz6X2SiImXRt/XEaqGDZkvjeLMdGXbeHNpv9iX0jmEK6xUwMvh+fkT6kB5flLVpb37VditkY44JRyz2GGYqu+b61jO+JCaS3JmCWDZdoDXlixMxuGNtkXEIwo4SZJQySicRFBYpr4mhoeI/P4owMjRfvMTMlndaqZtFJkc5t6BCZNsXBeooGzwPM5aOB5LKbtEMhzbkR4LjrOSn3TA+9+eabN4lA02n83d/93dKZDYKRBC/f9a53zVU0vg1tQH5NKOlHJ7TcA+poS/KppVpZ0Q4x2Lz22mvnJnBC/Sawnw4Q2Rf2o+ma5VLZZnB122231YNE6jcnJajf1PO8fdb8IQ/HTVHu00/0LXPaesyi71zuOhm3jAsS5P0x4xjGM9yQNg1U8NulceqsxkbjtqeLuG/HuGMWjnmox03pMo/6tkvsPyfYyS8e3LohTr7/h/+w8XC1yzHMvBmX71vD+J56TnlI25J5MNXgSRpN48FMFIIbXF7D4Pev/uqv6tdROWIgSOSQm8VEZaKA8DnL8V0GjkSqeT//LlP0Fnk607TQoR133HGbXX/JZU9NAzGuQ+VgnY4MdGY8mq5DVD+kL9MS0/TlngYMJppumsTggLRvOvNOtDatc3FglQdZ5h1tAAcS8RdxSDfSb5IDg/yAhWmi6T1kNAzx5+u+/OUvz91gkjMn3EE+Bji0tRxsxBnXVKls52dsY7Ym9Tw9U6P5RH9KuaB8gLwnUM6BGYPpPv1EnzKnrcc0+87QZ518xv18GMOA8skM2rhPSKk/5rv8WVvulZi3d23j1FmMjUrb04S6HCd82T/G33HfLtKBGQVRX9lu7oWxaPe26NMupTOQePAXV+P48c///M87H8MMWZ983xrG9zHuzu+JNPSbyG7RmSdkNpccpH+Bh4oRg2OCL+k9T9LPOGjk4JGCE1i2b+R3a0ZFpLOJKCYdQFwCkndwLMsALq5DZVmuW530TIU2RVlP05cBQExHTPOC53xGoxnLxiM/47LISBfSh7Rg30kL0o90RD4AK6Fsc2AS9YB0TqeCamWQD5R7yjwDCwYUnHWKfIpHtFFDRpv6mc98ph4YsM1p+0kZTc+YtpVtLS7KA+WC8pHmfRyYde0nUCpz2npsifalT3+cl3HKJ89j2VJ/THCGsQ8zEPgsHtFPbOlxatv2pH0Y4jIdlmH/4j3k9ZVjG4JKvL9I+vSFbcjvcccw86It3/uMZRcB9ZzjfG4STHrwoM4M/aTZqvUbjJ53tm7dutEzSZIkSZKk2Vq9evXo2crYojNPJEmSJEmS5o3BE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBUYPJEkSZIkSSoweCJJkiRJklRg8ESSJEmSJKnA4IkkSZIkSVLBqvUbjJ5LkiRJkiQp48wTSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqWCmwZM777yzWrVqVXX22WdXL774Yv3egw8+WB100EH1+/lnuOSSS5Y+O/bYY6unn3569MlG6ec8V3ekM+kd6TcujUOa1jzIN/JP03H11Vd3ygeky+b5wPf4fnzOsouoz37mZT1fNtomHpbrLS/yh3zogvybt/a+TxlrK9ulfjHvUxe1/i+qtvzjdXzW1k/Yrs0v8o42MR0PY1xbOc32ZRJ91tm2bFrG833Jv5v3A2k68EjXPYv9bhtb5NJ9a8rfwHKlz+fZcspqXu5LfeEQka95mW1r81Nt5X/e5fU3HoPvv9bPyFNPPbX+ne9853p+4iMf+cj6devWbfJe+rj44ovr73z3u9/d7DOW53tgufxzvqNuHnjggfUXXnhhnRdtSHPyLdJe03XHHXdsUrYpx1FPcnyWLpt+lwfPow6B54tWL0gX0if2K/abtMjFspEm+bLUg2OOOab+H3leaLYif2i/m/IvR56zbFrGh65PGWsr2+x32jawXLxm/QceeODSsqV6oeFpyz/+T8tNmve5PmVOw0Je0cbleRttQ95W9snrWEeXvrOrPuuMz8aNUfgOdaBpX+J34rv57/KdcfWn7XcnkW9Pab/Bb8W+gO+l2xNiP1g3v7FI+pTVSM80f9Pv5unJ6yGnGduXj10ir5vKbK6t/C+ifJ+HamYzT77xjW9Ut9xyy+jVRhsKSfX73/++2pAw1YYEqjYUompDIaojT7/97W+rn/3sZ/Vr3t+wbdWGAlKvg9dEoK6//vpqQyGr1xPf5TuLGKmdBdJtu+22q7bddtvRO+Ox7Pbbb1+tXr169I6mhfK6Zs2a6owzzqh23nnn+j0iy3/605+qxx57rH4dWJYyni578MEHV3vvvXddB3jgox/9aP0/TjrppOqmm24afES+j3vvvbd69NFH63QCaUGa/PSnP61fp0jDRx55pE4HsOxhhx22tOzatWurt73tbdV+++1Xv07TU7PFGZZoU2jLS+LsDHWFPmOe9CljpbJNHaZ/5HW022lb8Ytf/KI65JBD6vWjVC80PKX869NPwHZt/sRZ5Y997GPVhRdeOHp3o1JbOa32ZVJ91tk2RmE91IE999yz/mz//fevdtxxx3oMynECnx955JH1Z7SBhx9+ePXwww/Xr0mHE088se7fwXb8+Mc/rl+3/e4k2sYWqaaxG/tBe57+PstdeeWV1THHHDN6Z7H0KavxXpQrysQ+++xTt5Mg30nvSM83v/nNdXtIORmS0tilT5/dVv4X0fe///16n9N6O0QzCZ7Q6H/uc5+rvvWtb9UBjjZUpNIBPQlJQ/rLX/5yqSGiIt53333VN7/5zU7BAG1Mx3/+539emhZFAzWuEyG96SjovOdiCtUciQaRch9I5x122GGpkwiUbcr46aefPnrn5e8H1pMHuZ555pk6DxdFpFe6n7zOByKgbSDoGp01n7McHRADFQK1r3/96+vPQBrvvvvuyxpMqpvXvOY1dbn8whe+MHqn7KqrrqoHw+TPvOhbxkplG3EwEEg/6jdoF/I+cJEHVoumlH99+gnbtfn1yU9+sh7L5mPlcW3lNNuXSYMIfdeZL4sYo/DZ3XffvRQQ5AD62WefrXbZZZf6O3we+8W+E5Bg39N+fZzS706iNLbIkSf52I39SA/+wcEied/lWGne9C2rpA39XaRPBKsIkoD1pGWMdpD2MM/jIRg3dunTZ5fK/yIiXwk4XXTRRZvUkSGaevCEA+yvfOUr1cUXX1wdccQRo3c3osEhuk5QhUJBhBnnnXdetdNOO9WFjAAJ73PA/v73v7/+nIJFYx3P4+B/Ua8PnBXS7lWvelU9KGNmD4EoKnJTZ0d6//GPf6w7GZb9+te/Xp188skGUKaEMysMDkJ0KF3QINOgErmmrpBXdMBh7dq11ZNPPjl6tThIn7TDIf1Ix3FoG2gjWI5OOT0AJf1Si9oZDc3RRx/duVOkvyBoO6/6lLE+ZZv6nZ7JS9E+33HHHUtnRjVf8vzr20/Yrs0X2kLaxCZtbeWs2peuuq6zbYxCv3zdddfV40vG9Rw8/eAHP6j3nfVzoAk+47ghxq2I3+NEYBwXcHCNWY6NSmOLJpxQjm1Lz6gz9uZgOGZaLKq+7RLpQpqQhx/4wAeW+jrynQNr0p305JgmD0QMQZ+xS6nPbiv/iyY9thm6qQZPaFAuu+yy6rWvfe3YKTd5hI1gSUSBqRQEXQKBljway4yWQBCGwIsBlG5I37Shed/73lcXVApsjsqZRoDpHJgeSeejlUMnTHCSck8+kj+XX355/V4MHg444IDqDW94w+gbW6/oeAj+0VkbbNW842Zx+QA8MAjjACQdbGp+mH9aRG1jFMY05557bnXbbbctndQ79NBD6/oQB9H033zGiT+CK3HTTGao8Jx18DmzVrgEijZylmOjvmMLxtMsy/acdtpp9b6B2xvEbHq9jPTg+IM045gx8pv/yf84AUzal2bQD11bm99W/hcJ9YdAIpclxTHqkE01eMJ1kAQ0mNZGZJCoIcER3nvve99b3XzzzXXwgwAJBYGgCddyUhCi8HOAz2c8TjjhhPr7FJyIXObfTaf7qR8KaOksVo580HTQ6adTR2k4mN5YEoETztKkDS3PmfYb9YZ6B+rgIiF90gEK6Uc6dsG0T6Z/Pv744/XrmMkW8qCutFx9yliXsk0/yUEB7UA+2I5BGCccFvWs1CIbl399+wnbta3HtNuXvvqsc9wY5c/+7M82uy9IeqIuvwcGY1ZOHHGm/l//9V/rmScEJOK7/A7fjcsctsTYKMYWXY5D0nt4xAyZtlkri2A57RKXRJFWDz30UP0/ZSUOrksngIeuS59dKv8RgFsU+SVaQzezG8Y2iQOXHDeRpeFlcEh0OBqVaACpHDR26SyUuBZM3UQEM9IWMRDL05H3iaQzSE/R4BlAWb4ou2mHQkSZG1+Nazgi4s7ZmTRw0mTI14FOKtKLdAq8ZuCRH0hSxsedjdhmm23qgGHaeUc9KF07LXUVQemuZaytbEd7zPdjOnuK8s5BAZdWGjiZP+Pyr08/0bfMaX5Nu32ZxHLXGWOUOAieBH0560iDi22WOzYqjS1yTWPuFMc3nEyOWTHcpiBONHdZ/zzoW1Y55tgaZgjbZ2+OuklgMW4cPXRTDZ7QcEaElwdRMwIe3G2YQd9f//Vf16+jwSAgwiyVuNN2VCau5+ZzluO7XP8UEeX4Lo0fDU16l26NR4dG/nAAHg1TXAuaX19Gg8fUKZaNRpzKHh2Hlof0JXqepi+Rczr1prJMh0LaN51xzjtootHMTkmj84uAMsqALc4wsN+kX1MHnC9LeedywrhPBNeWppH7+MsBcVZKWq4+ZaytbF966aX1/5ThvE6zfqap81u075ovpfzr20/Yrm09ptm+TKLPOvls3BiFex02jTX5y5rsY+xPU1/OsQTrSGeus26+y3aUfnfSsVHb2CLFWC3qbzrmJp/YrnSWPQ/+umgcK00a1BqiPmWVdOEzlgF5SP6Sjvvuu2/jMUyk57wgHbr22aXy33YSdd4QYCPQNjfHLRsq7cw8MPp71hsahKW/wx3v8dM80s/A33Ye9xnPeW/c52qXpu87k7+Xzv+83lCh69eIv1HOgzwj7zQ9afqOy4t4Hsulj/hb73mdivcXTZ4W6X7yPG0P8mUp9ynSNj6zbG95aRkPPOc9PsuRf3keDt24Mhb9WFp+x5XtvG6nD9aftufpw75xPnTJP8pCvJ/Wj3F1KJa1XZsveR8WmvIZ02hflqO0znxf8nYs/31ex2c80n3NfydPozQd8u+2/e4k8u1J+yV+m89YJqR1PP8slafZIulTVtvyLE3PdF1DxfamZSTd/vQReZ+Xg7byvwiaysHQreKfDRkiSZIkSZKkBlv0nieSJEmSJEnzxuCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqWDV+g1Gzztbt27d6JkkSZIkSdJsrV69evRsZTjzRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBUYPJEkSZIkSSoweCJJkiRJklQw0+DJXXfdVb3yla+sPv7xj1cvvvhi/d5DDz1UHXLIIfX7+Wf40pe+tPTZCSecUD3zzDP1+9ddd93S++mDdbFOdZOnI3k0TuSf6TwbaV6kZb1Juuy4vKAeUZ9KeTrPSB/SKdKBNBkn0mLcsl3SU9OX5wvtfcm4/mAe9Gk/28p2KR361AsND+WC8jEu//rUAfvsxdFWr/v0YbNoI/qsczn70vV3om9p+nyaY6O2sUWuT9vNeln/ollOX5jn2TyOCSgjbHeObf/gBz/YKz2a1jPv8jo1D/k6s+AJO55nMu9dcMEF1a9+9avRO1V15ZVXVpdffnn9nAJ2ySWX1M/xk5/8pPrQhz5UTMT3vOc91b777jt6pRIaocsuu6y67777qhdeeKH+/4tf/GJjxeU9Potl+R55N/QCPS/Ii2uuuaZ6/PHH6/Q97bTTqs9//vONHSf1Il22KS/43qc+9am6Pi0i9o/0IZ1IA9KCNGkaDEVa7Lbbbo3L5vXAsr3l0NZHvjz11FPV7373u7GDT94nr7rUkaHp0362lW36UdKJ9MrToU+90PBQTs4444y6fPTN+5x99uIgzxj7HnrooXVe8njkkUeW2so+fdgs2og+61zOvrR9N/XDH/6wcfzDtk5rbBTrGje2yHVpu9N9Y71xPLQo+rRLkd9RrvhOeozSpz0cCsrqmWeeOXr1stjXBx54YPTO5vIy0jZmmldR5ucpX2cWPLniiivq4Efq6aefrp588snqrLPOqhOJivGmN72pbngoEHfeeWf9OirZt7/97XodDz74YHXyySfX78WDz97xjndUH/7wh0drVwmF8Hvf+1513nnnLQWb+J8Opyn4dP3111dvfetblz476KCDqr322qvOCy1P5AUNxE477VS/d/TRR1fPPfdc9cQTT9SvA8tSL9Jl87ygId1ll13q59SJRUSb8Jvf/KZOJ5AWpMntt99ev06Rho8++mh14okn1q9Zls4nln3ssceqt7zlLdUee+xRv95vv/2qHXfcsW6fNDsMgH7+858v5cu2225bnXrqqXX5buokGSSTb1HuDz744LqODLlDDX3az1LZZoBF/0g6kV5I24o+9ULDc++999ZtEeUDffI+Z5+9OCLP0vEt7eaPfvSjulz06cNm0Ub0Wedy9qXtu4G+hX7kv/23/zZ6Z6Npj43axhaprm13um+f/vSn68ci6dMuxXtRrigTe++9d91O9m0PVxplklk2BNc45k0RBHrd615XvfGNb6xe85rXjN7dHGMdysgRRxxRv2a/DzvssHpstChiXHjOOecs5SvH+1/96leXXg/RTIInNFjMIPna175WB0PaUJFKiUTjmiKxiV7SWNN4qV00+hyAtKHCEszaZ599Ru9srLRExR2UL180iHvuuefonY3pu/3229edRIr3aURoTEJ8P7z61a+uz4AQqV1UtAF5O0H60ZmmgyjQSadBweh0owPie/fcc89Sh0uH/eyzz1Y777xz/VqzQcB81apVm6QzA1vapabBD+1Pmr/UDepIqa8Ygr7tZ6ls48Ybb6wH6IGDikiTPvVCw9M0SIyBMWObUt6n7LMXT9O4mLynDPTpw2bRRvRd53L2pfRdUPavuuqq6r//9/9e9w+paY+N2sYWqbb6S71MTw4sor7tEulBmkWapMctfdrDoWASAdvM/qYImjAe4pKdEtKK8h9pRXoSJEzTc94xriNAFsHTeTH14EkENi666KLq8MMPH727EQ0OMx+YPsegOc62EHEi0kwB45Ie3ue6p5jqlEfZiGTuuuuuS9FJdUPjQ4UlGkr6tl17SKeWWqQKu9Io73FGBNGhdHHrrbfWDWrUnyOPPHKhO+BA+qSDKNKPdByHjobrKInw0+FGp8v/a9asqafLUw+YBXTttdduFWm40vKBMAPkcenOgSVn4cg/8ol+YOhnI1J92s8+ZTs/k9e3Xmi44ixcnNnO5Xmfs89eDMy+4AQJB+qBvP/DH/5QP+/bh82ijei6zuXsS9t3wQyO7bbbrrFOzGpsNG5s0Savv9RPZiGw3zx4voj6tksERLjvBWPc008/fWx719YeriS26YADDhi92tTxxx/faRzDMox5QPmgjjHzJj2ZughoS6jjUQ/m4d4/Uw2esLPMNiGwMe5ymjwQQrCEA3owUCboEj7xiU9sNnOFwcUNN9zgrJMJcG0dHdM//MM/1Jc+EeSiwyoFUDQszOoi39IpbmoWHQ9lnc46GmTSkGtub7755vozOqO3v/3t1oOBYSBJe0X/EHk4LzeImxXShDOd4/pXzS/aH/rjcQcL5v3Wg7HtpZdeWvf1cUDB2er999+//nye+rDl7Evbd+kL6CO2dJ0YN7Yoaaq/nCBm1grroZ9jlsaiBlD6IN+ZsUG6cMzYlCZbQ3sYQSTKWJQRyvuilZG41yn7yINgCvcWGnIAZarBEyLAzCrhPiVEZIkaEhzhvVNOOaW65ZZb6kQiQEICMZ2O6xApCDEgJoASCUh0ju+nUUqm+PBeHslUOzocpi/GQTf5w7Wm+aUigamZqTzwpckxLTWChqCRoOMsicAJZ2mGGGmfNdInbUxJP9KxC6Z9Mv3z3/7t3za7hwxnjbjxNGcxNFucRUzzcNy0W95jYJRe3/zud7+7nrnCzKt50Kf97FK2Y7D4ne98Z6nsYjn1QsMQgRNm5jadVRyX9zn77MVBH3/33XcvjYeZhQECCH37sFm0EX3WuZx9GfddZi1yWQR9RKlOzFqMLZouPQ3j6i/3boxZK/Rz7AvLNfWJ82w57RLBpTxNuraH8y6/BwxlhBOnzE5cpJN9xATSfo+Zl211aqXN7IaxTcYlBDeRZRBNhaAxpVIgrvOKQAkNNQ0tAZdoQNVNXD8a14mWUEGJ/KUNHGlPZ9l0baf6IX05CEw7FNKXG1+NuycNdYMbT3F2ZmsMnNAG5AfepB8Dj7zzpP3Y2mcoDBFTTtevX79JG8SAex6vdy3p2362lW3e58wm38+n5vepFxom2itOZBAYzwMnpbxP2Wcvvknv+TSLNmK561zO/aviu/w2dYfjAY4b6F84UctsDsZLs9B3bFGqv5wUXvTgZt92iROEpVk8pfTUfKItIT/H5flQTTV4QsMZ0WEezEThshvuNExB/6u/+qv6NbNPaOyYncIslbjTdlSmaAxZju/GvR1IXBrs/Lp5taOROe644+rLqqKQkj/cqKvpgJ3IXxrdjDuDG7RaPsouZxgIhkQnzNl0BgRNB5F0KHTaix5lL6ENSGcdkG6kX1MHnC9Leafcc20sU365W3ma9qQtlwKOu8+ApoOgH3kQZxTJF6agkh95e045pz/h82ivuCaWNmge7nXVp/1sK9vxZ/y+/OUvb5ZOfeqFhofywWwTxkGU91wp73P22YuDesxBOn0TyFOCa4wbuK9Inz5sFm1En3UuZ19K3919992XLu3gQSCe4wVmc8zqL9aUxhZNJ7VK9Zd+jP2KfYv+cDlBrSHq0y6RJnzGMiD/CYTFzKQ+7eEiiDTqWt7mEXUK+X2Nhn5SbYvOPCGzueQgvY8JjV1UBBqN9J4n6WfgjCWzVDQZzmrRURGhJzjFoI38IF/yTor3PvOZz9QFO5bl2tOt9eB92ijrdAhxM0wGD3FJVZoXPOezuBSOZeNBUGVrQbqQPqQF+05akH5xwJGesciXpbxz9iMGVNSDNO0J1jIgW5TOaMjOPffc+ixDmi9xtp3yTrmnzIP84vNor8ijeWmDSu0nZZSyGvW3VLYZcHIgETdZ5/N4kF5t9ULDxiCRy5DjhFE8KB+8X8p7++zFRZ6Rd+QheUme8jzqdakP69O+TKpPf7ycfWn77pbWNrZI+7C2tpt944RYzLjP17Uo+vSFeX5HuaKMtKXnosjrDmUkLW9gHLUoqFMc53NVSeQnY8ShB8hWrWcedU/r1q0bPZMkSZIkSZqt1atXj56tjC0680SSJEmSJGneGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFaxav8HouSRJkiRJkjLOPJEkSZIkSSoweCJJkiRJklRg8ESSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJBQZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVDCV4MmLL75YnX322dUll1wyemejp59+ujr22GOrVatW1Q+WYdnUgw8+WB100EFLy1x99dWjT17Ge/E5jzvvvHP0ydYl0jnf/3h/S6ZR5O3WmheSJEmSpK3HVIIn3//+96tvfetbo1cbcUD/2c9+tjrssMOq9evXV+vWravfP++885YCKAROTj755OrCCy9cWuZnP/vZJgEUDs6/8pWvVA888EC9zB133FG97W1v2yoP2i+99NLGdCZNQfpt7WkkSZIkSdK0LTt4QgCE4Ebu3nvvre6+++7qpJNOql9vu+229UH+I488Uj322GP1e2vXrq0OOeSQ6n3ve1/9Opb5X//rf9XrJTCwZs2a6sQTT6z222+/ehmCMRdffHH105/+tH69NYhZHp/73OdG77yMtCRNSTfSD5FGpF0EqiRJkiRJ0mSWFTzhwPyyyy6rPvrRj1Yf+chHRu92w3d/+9vfVocffvjSQT922WWX+rKTp556avROP6yXS1iuuOKKpcuBYgZGfolQXEYU30kvO2p6j++/853vrP8Hn8W6eDRdcpTi83w2CO/FdjQhcHL66adXe++9d/WP//iP1YEHHjj6ZCOCSrfccstScCm8/vWvHz0bb1x6hPxyqdL+Ne0H+8r6+Z0IAN14442bXMpFGuaXd6VpFOslP+PzWGfI94N1sU5JkiRJkqZhWcETLtd59NFHq3e9612jd1528MEH17NK1q5dW7/moJpAyz777FPtueee9XslrJegyhlnnFFdf/31SwfLHFgzA+PII4+sX4/zjW98o7ruuuvqy1iYicH39t9//00uEUJc8kIQh2BOHPwzo4OZM+l7BHQ4OCfAw0E/n8WlMlxWxAycPDiSIgDC5TQRhOB/LlMiXdIAUmrnnXeufvzjH1ff/OY3xy6TY3tZ7+677z72O5EeX//61+vt58HypAffZ//Sy6Vi/0oBlC7+9m//trr88svrdX73u9+t8/Koo47a5L2PfexjmwRHuFTpySefrD8nvSlXpBnbyXJc+pXuB/nNJWORb5IkSZIkLcfEwRPO7HNZyEUXXVQf4Oc4aOeAHwQcVq9eXR+cRxCAB685yE8PcglQ/P73vx+92ngJCkEQDpBZDwfWHMjzfkl6qQ/rZ1u5lIVZHOD3CRQQIOESIwIbPI9Lin7xi19Uu+66a/WnP/1pKdDCpUL8LvtC4CQNTvBb9913X3G7+Iz7kbz//e+vTjnllHqbvvCFLyytY1rYHwIOpQAT+0J6pNtLXpI/pAEBKwISkYb8T+CJbV7OrI40X9785jfXM2lYb/oe0plHzPZhdhNIKwJqkVexHAGtEPsx7XSVJEmSJG2dJg6eMLODA+9xwYK4lILLR2JGANJLKji45yCfGSwgyMGMgl/+8pf1azAD4txzz61uu+22eh0RSGmbAZFetkLwg5ks+aUszIBhFgOBBGZhvPa1r106GH/44YfrA/Znn322DtawzczWYJs5KGemCrMm+l4iEgGUP/7xj/U+NAWeloN0J8CUB0ZSsS/jLu1pCkiAwAaBLdJjUulvsn7SnMBVCZ8TsGoS+cb/y50VI0mSJElSk4mCJxx484jZAE3WZjeDRSzPZSiIQAIzMZhVcuihh9azCphpAAIBzIBIZ7csZwZE6SCd9bM9BFJYLzNLuMSG9wi8EFDYcccd64N0MIOFbed+I3GfltK9SwLpxnpf9apX1evouw8lcQkLsztIszal9CAgkQdPhoh8+8EPflDfcyfKEQ/SWZIkSZKkaZgoeEKAIQ0aMCuAGST5TIz8nhssxwE7szoCwYmYmcJlL6yT2Q1xYM+lM/lBPJ+xTHppRxcEQZrEbAj+J2jy+OOPV9tvv329vbzHpUW33357tcMOO2wyAyLddgIppAF/TnicCJyw7LXXXlsHiqZ1b46+gROMSw+MS9+m/FhplDEu0yEfmGVEICW/b4okSZIkSZOaKHjCwXkEDdIDVi4VYVZJzBJJb7aaimAFl+TkszW410hchgFuFNp0EN9nZkRT0AZxU9gI1HBZCn/295prrqnvxcFBOZ+xDDNg8r8MlCKQws1Ox+0z7rnnnk3u18LME9YZN2mdVN/Aybj0CJGuebqTNwTLxqU7wZi4PwxKwZlZIX+4jwzlo29wTZIkSZKkJhPf86TNSSedVAcd4n4mYFYGB9TMTkHc84QbnIIgAH/RhRkZBGC4RIeAQPw521iGWQWxTBccULM8M2PivhgEK7i/CpcW8ZeBwD1Q+GtAbEMEVOKeGgRx4mamfJegD9sV2D4uJSoFWAiSxI1RAwGU5dzclN/lnjB9Zpyk6ZFe3kLakDcER1hfOnsjz5sc6cVspLgfSiw/a2x//qeL47KwCMBJkiRJkrQcMwueECTgJq8EFNL7UKQ3SWUGBpewcCkLn3Owy194ib+IAwICHLBzQB/LcM+TdJku+C0O7DmgZz3Mvkj/+g/4n/e450oceMcsDYIs8SeWWY7AC7NMYt/YPn6j73YtF4ECghYEQmJb4lG6mW2kBwGSWJ68ivwh3Uln0oHPmvImxfqYeRN5SUCH5bnMZ5b4XX4ntpMHecyfPu4aXJMkSZIkqWTVeq67kSRJkiRJUqOZzTyRJEmSJElaBAZPJEmSJEmSCgyeSJIkSZIkFRg8kSRJkiRJKjB4IkmSJEmSVGDwRJIkSZIkqcDgiSRJkiRJUoHBE0mSJEmSpAKDJ5IkSZIkSQUGTyRJkiRJkgoMnkiSJEmSJBUYPJEkSZIkSSoweCJJkiRJklRg8ESSJEmSJKnA4IkkSZIkSVKBwRNJkiRJkqQCgyeSJEmSJEkFBk8kSZIkSZIKDJ5IkiRJkiQVGDyRJEmSJEkqMHgiSZIkSZJUYPBEkiRJkiSpwOCJJEmSJElSgcETSZIkSZKkAoMnkiRJkiRJY1XV/w9lSzAG8qsTEwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "c0bf747c",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ce7e12c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# showGistogram(filt_df_norm)\n",
    "# showBox(filt_df_norm)\n",
    "# showScatters(filt_df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7072211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>0.064584</td>\n",
       "      <td>-0.936791</td>\n",
       "      <td>0.271723</td>\n",
       "      <td>-1.361716</td>\n",
       "      <td>-0.138380</td>\n",
       "      <td>0.087359</td>\n",
       "      <td>0.741858</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>-0.932970</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.059021</td>\n",
       "      <td>0.618562</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.334065</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-1.025751</td>\n",
       "      <td>0.502191</td>\n",
       "      <td>0.451354</td>\n",
       "      <td>-0.484375</td>\n",
       "      <td>-0.414378</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2</td>\n",
       "      <td>-0.504606</td>\n",
       "      <td>-0.120075</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.833316</td>\n",
       "      <td>1.230708</td>\n",
       "      <td>0.690750</td>\n",
       "      <td>-0.057028</td>\n",
       "      <td>-0.876763</td>\n",
       "      <td>-0.587242</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.509923</td>\n",
       "      <td>-1.597349</td>\n",
       "      <td>0.253252</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-0.036966</td>\n",
       "      <td>0.426767</td>\n",
       "      <td>-0.129654</td>\n",
       "      <td>-0.190083</td>\n",
       "      <td>-0.846538</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.1</td>\n",
       "      <td>-0.617331</td>\n",
       "      <td>0.895551</td>\n",
       "      <td>-0.958829</td>\n",
       "      <td>-0.039983</td>\n",
       "      <td>-0.430490</td>\n",
       "      <td>-0.214440</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>-0.386277</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>5.7</td>\n",
       "      <td>-0.730057</td>\n",
       "      <td>-0.120075</td>\n",
       "      <td>-1.091056</td>\n",
       "      <td>-0.414030</td>\n",
       "      <td>0.234671</td>\n",
       "      <td>-0.417268</td>\n",
       "      <td>-1.142051</td>\n",
       "      <td>0.306102</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-0.504606</td>\n",
       "      <td>0.433903</td>\n",
       "      <td>-1.002904</td>\n",
       "      <td>-0.788077</td>\n",
       "      <td>-0.363974</td>\n",
       "      <td>-0.645449</td>\n",
       "      <td>-0.297303</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.496597</td>\n",
       "      <td>-0.760106</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>6.2</td>\n",
       "      <td>-0.730057</td>\n",
       "      <td>-0.397064</td>\n",
       "      <td>-0.936791</td>\n",
       "      <td>-0.351689</td>\n",
       "      <td>-0.696555</td>\n",
       "      <td>-1.152519</td>\n",
       "      <td>-0.991203</td>\n",
       "      <td>0.523980</td>\n",
       "      <td>0.104208</td>\n",
       "      <td>0.536374</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.509923</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.147041</td>\n",
       "      <td>1.498477</td>\n",
       "      <td>0.774345</td>\n",
       "      <td>0.426767</td>\n",
       "      <td>-0.347532</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>-0.846538</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>6.5</td>\n",
       "      <td>-0.391881</td>\n",
       "      <td>-1.320360</td>\n",
       "      <td>-1.024942</td>\n",
       "      <td>-0.227006</td>\n",
       "      <td>-0.297458</td>\n",
       "      <td>-0.670803</td>\n",
       "      <td>-0.463236</td>\n",
       "      <td>-1.509548</td>\n",
       "      <td>-0.288180</td>\n",
       "      <td>-1.019402</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3371 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.3          0.284472     0.064584       -0.936791   0.271723   \n",
       "1               8.1          0.059021     0.618562        0.231214   0.334065   \n",
       "2               7.2         -0.504606    -0.120075        0.583819   0.832794   \n",
       "3               6.2          0.509923    -1.597349        0.253252   0.022359   \n",
       "4               8.1         -0.617331     0.895551       -0.958829  -0.039983   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "3366            5.7         -0.730057    -0.120075       -1.091056  -0.414030   \n",
       "3367            6.5         -0.504606     0.433903       -1.002904  -0.788077   \n",
       "3368            6.2         -0.730057    -0.397064       -0.936791  -0.351689   \n",
       "3369            6.6          0.509923     0.249243        0.473630   0.147041   \n",
       "3370            6.5         -0.391881    -1.320360       -1.024942  -0.227006   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -1.361716             -0.138380  0.087359  0.741858   \n",
       "1               -0.297458             -1.025751  0.502191  0.451354   \n",
       "2                0.833316              1.230708  0.690750 -0.057028   \n",
       "3               -0.297458             -0.036966  0.426767 -0.129654   \n",
       "4               -0.430490             -0.214440  0.011935  0.160850   \n",
       "...                   ...                   ...       ...       ...   \n",
       "3366             0.234671             -0.417268 -1.142051  0.306102   \n",
       "3367            -0.363974             -0.645449 -0.297303  0.669232   \n",
       "3368            -0.696555             -1.152519 -0.991203  0.523980   \n",
       "3369             1.498477              0.774345  0.426767 -0.347532   \n",
       "3370            -0.297458             -0.670803 -0.463236 -1.509548   \n",
       "\n",
       "      sulphates   alcohol  quality  \n",
       "0      0.006111 -0.932970      6.0  \n",
       "1     -0.484375 -0.414378      6.0  \n",
       "2     -0.876763 -0.587242      6.0  \n",
       "3     -0.190083 -0.846538      6.0  \n",
       "4     -0.386277  0.363510      6.0  \n",
       "...         ...       ...      ...  \n",
       "3366  -0.288180  0.017782      6.0  \n",
       "3367   0.496597 -0.760106      5.0  \n",
       "3368   0.104208  0.536374      6.0  \n",
       "3369  -0.288180 -0.846538      5.0  \n",
       "3370  -0.288180 -1.019402      6.0  \n",
       "\n",
       "[3371 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a5616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filt_df_norm.loc[:, filt_df_norm.columns != 'quality']\n",
    "Y = filt_df_norm['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32a5e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5efa483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# валидационная доля от тренеровачных данных\n",
    "val_part = (samples_count * 0.3 ) / (samples_count * 0.9)\n",
    "val_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "864c6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1d91e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренировачная выборка:\n",
      " 6.0    1401\n",
      "5.0     891\n",
      "7.0     538\n",
      "8.0     104\n",
      "4.0      90\n",
      "3.0       6\n",
      "9.0       3\n",
      "Name: quality, dtype: int64\n",
      "====================================================================================================\n",
      "Тестовая выборка:\n",
      " 6.0    153\n",
      "5.0    104\n",
      "7.0     61\n",
      "4.0     10\n",
      "8.0      7\n",
      "3.0      2\n",
      "9.0      1\n",
      "Name: quality, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Тренировачная выборка:\\n', y_train.value_counts())\n",
    "print('='*100)\n",
    "print('Тестовая выборка:\\n', y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d964bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренировачная выборка:\n",
      " 2022.0000000000002\n",
      "====================================================================================================\n",
      "Валидационная выборка:\n",
      " 1011.0\n",
      "====================================================================================================\n",
      "Тестовая выборка:\n",
      " 338\n"
     ]
    }
   ],
   "source": [
    "print('Тренировачная выборка:\\n', y_train.shape[0] * (1 - val_part))\n",
    "print('='*100)\n",
    "print('Валидационная выборка:\\n', y_train.shape[0] * val_part)\n",
    "print('='*100)\n",
    "print('Тестовая выборка:\\n', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30ffbe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3371.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0] * (1 - val_part) + y_train.shape[0] * val_part + y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bdb1d",
   "metadata": {},
   "source": [
    "## Многослойная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1c6ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_speed(epochs, model, loss_fn):\n",
    "    weights_1 = []\n",
    "    weights_2 = []\n",
    "    weights_3 = []\n",
    "    weights_4 = []\n",
    "\n",
    "    gradients = []\n",
    "    all_weights = []\n",
    "\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "        all_weights.append([])\n",
    "\n",
    "        # Iterate over the batches of the dataset.   \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "            weights_1.append(model.layers[1].get_weights())\n",
    "            weights_2.append(model.layers[2].get_weights())\n",
    "            weights_3.append(model.layers[3].get_weights())\n",
    "            weights_4.append(model.layers[4].get_weights())\n",
    "\n",
    "            for i in range(1, 5):\n",
    "                all_weights[epoch].append(model.layers[i].get_weights())\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(tf.constant(x_batch_train, shape=(len(x_batch_train),11)))\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            loss_values.append(loss_value)\n",
    "            gradients.append(grads)\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_logits = model(x_val, training=False)\n",
    "        loss_value_valid = loss_fn(y_val, val_logits)\n",
    "        val_loss_values.append(loss_value_valid)\n",
    "    #     print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "    avg_global_speeds = list()\n",
    "    max_global_speeds = list()\n",
    "    min_global_speeds = list()\n",
    "    for epoch in range(epochs-1): \n",
    "        avg_speeds = list()\n",
    "        max_speeds = list()\n",
    "        min_speeds = list()\n",
    "\n",
    "        for layer in range(5): \n",
    "            try:\n",
    "                speed = (all_weights[epoch+1][layer][0] - all_weights[epoch][layer][0])/gradients[epoch][layer*2][0]\n",
    "            except Exception as ex:\n",
    "                print('--------------------------------------')  \n",
    "#                 print(f'layer = {layer}, i = {epoch}')\n",
    "#                 print('--------------------------------------')\n",
    "#                 print(len(all_weights[epoch+1][layer][0]))\n",
    "#                 print(len(all_weights[epoch][layer][0]))\n",
    "#                 print('--------------------------------------')\n",
    "#                 print(all_weights[epoch+1][layer][0])\n",
    "#                 print('--------------------------------------')\n",
    "#                 print(all_weights[epoch][layer][0])\n",
    "#                 print('--------------------------------------')\n",
    "#                 print(gradients[epoch][layer])\n",
    "#                 print('--------------------------------------')\n",
    "#                 print(ex)\n",
    "            avg_speeds.append(tf.get_static_value(tf.reduce_mean(tf.reduce_mean(tf.abs(speed), axis = 0), axis = 0)))\n",
    "            max_speeds.append(tf.get_static_value(tf.math.reduce_max(tf.math.reduce_max(tf.abs(speed), axis =0), axis = 0)))\n",
    "            min_speeds.append(tf.get_static_value(tf.math.reduce_min(tf.math.reduce_max(tf.abs(speed), axis =0), axis = 0)))\n",
    "        avg_global_speeds.append(sum(avg_speeds)/ len(avg_speeds))\n",
    "        max_global_speeds.append(max(max_speeds))\n",
    "        min_global_speeds.append(min(min_speeds))\n",
    "\n",
    "    speeds_0 = list()\n",
    "    for i in range(epochs-1): \n",
    "        speeds_0.append(tf.get_static_value(tf.abs((weights_1[i+1][0][0][0] - weights_1[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "    speeds_1 = list()\n",
    "    for i in range(epochs-1): \n",
    "        speeds_1.append(tf.get_static_value(tf.abs((weights_2[i+1][0][0][0] - weights_2[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "    speeds_2 = list()\n",
    "    for i in range(epochs-1): \n",
    "        speeds_2.append(tf.get_static_value(tf.abs((weights_3[i+1][0][0][0] - weights_3[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "    speeds_3 = list()\n",
    "    for i in range(epochs-1): \n",
    "        speeds_3.append(tf.get_static_value(tf.abs((weights_4[i+1][0][0][0] - weights_4[i][0][0][0])/gradients[i][2][0][0])))\n",
    "        \n",
    "    return [speeds_0, speeds_1, speeds_2, speeds_3, max_global_speeds, min_global_speeds, avg_global_speeds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb0f83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer, kernel_initializer, bias_initializer):\n",
    "\n",
    "    # инициализация модели\n",
    "    input1 = layers.Input(shape=(11,))\n",
    "\n",
    "    # скрытый слой\n",
    "    x1 = layers.Dense(\n",
    "        20,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        bias_initializer  =bias_initializer)(input1)\n",
    "\n",
    "    # скрытый слой\n",
    "    x2 = layers.Dense(\n",
    "        10,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        bias_initializer  =bias_initializer)(x1)\n",
    "\n",
    "    # скрытый слой\n",
    "    x3 = layers.Dense(\n",
    "        10,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        bias_initializer  =bias_initializer)(x2)\n",
    "\n",
    "\n",
    "    # выходной слой\n",
    "    out_x = layers.Dense(\n",
    "        11,\n",
    "        activation=tf.keras.activations.softmax,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        bias_initializer  =bias_initializer)(x3)\n",
    "\n",
    "\n",
    "    # Соберем полную модель сети от входа к выходу\n",
    "    model1 = Model(inputs=input1, outputs=out_x)\n",
    "\n",
    "\n",
    "    # Компиляция модели\n",
    "    model1.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=tf.keras.metrics.MeanAbsoluteError())\n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138106a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train).astype('float32').reshape((-1, 11))\n",
    "y_train = to_categorical(y_train, 11)\n",
    "\n",
    "x_test = np.asarray(x_test).astype('float32').reshape((-1, 11))\n",
    "y_test = to_categorical(y_test, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "379332ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # инициализация сети\n",
    "\n",
    "# kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2065ab",
   "metadata": {},
   "source": [
    "### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd86133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "# optimizer1 = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "# optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "# optimizer3 = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "# optimizer4 = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "# optimizer5 = tf.keras.optimizers.SGD(learning_rate=1)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4, optimizer5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdd8eb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 1.095:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e59c7a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 1.05:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af335f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 1.06:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b77e1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 1.07:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5dafebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# errors5  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model5 = build_model(optimizer5)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 1.14:\n",
    "    \n",
    "#     hist5 = model5.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors5['epochs'].append(epochs)\n",
    "    \n",
    "#     errors5['train']. append(hist5.history['loss'][0])\n",
    "#     errors5['val'].   append(hist5.history['val_loss'][0])\n",
    "#     errors5['test'].  append(model5.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist5.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist5.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c632032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score5 = model5.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Model-1. lr=0.0001 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Model-2. lr=0.001 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Model-3. lr=0.01 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Model-4. lr=0.1 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Model-5. lr=1 ***********\")\n",
    "# print(f'Epochs: ', len(errors5['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist5.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist5.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist5.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist5.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score5[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score5[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03e93d09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 3, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Модель-1, lr=0.0001', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 3, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Модель-2, lr=0.001', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 3, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Модель-3, lr=0.01', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 3, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Модель-4, lr=0.1', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-5\n",
    "# plt.subplot(2, 3, 5)\n",
    "# plt.plot(errors5['epochs'], errors5['train'], label='train')\n",
    "# plt.plot(errors5['epochs'], errors5['val'],   label='val')\n",
    "# plt.plot(errors5['epochs'], errors5['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Модель-5, lr=1', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/GD/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146611f5",
   "metadata": {},
   "source": [
    "### GDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aadfe460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0)\n",
    "# optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.3)\n",
    "# optimizer3 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.6)\n",
    "# optimizer4 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f6d519b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 0.1:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ff97e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer2 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer2   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2, kernel_initializer2, bias_initializer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 0.1:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3221a9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer3 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer3   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3, kernel_initializer3, bias_initializer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 0.1:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8184b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer4 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer4   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4, kernel_initializer4, bias_initializer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 0.1:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55457e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** GDM. momentum=0 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** GDM. momentum=0.3 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** GDM. momentum=0.6 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** GDM. momentum=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22563fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 3, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'][:-1], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('GDM. momentum=0', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 3, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('GDM. momentum=0.3', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 3, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('GDM. momentum=0.6', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 3, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('GDM. momentum=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/GDM/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363607a",
   "metadata": {},
   "source": [
    "### NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c08d7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0,   nesterov=True)\n",
    "# optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.3, nesterov=True)\n",
    "# optimizer3 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.6, nesterov=True)\n",
    "# optimizer4 = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75e540a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "539540f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer2 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer2   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2, kernel_initializer2, bias_initializer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98594b07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer3 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer3   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3, kernel_initializer3, bias_initializer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "511561d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_initializer4 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer4   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4, kernel_initializer4, bias_initializer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64a110e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** NGD. momentum=0 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** NGD. momentum=0.3 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** NGD. momentum=0.6 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** NGD. momentum=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a452d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 3, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('NGD. momentum=0', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 3, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('NGD. momentum=0.3', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 3, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'][:-1], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('NGD. momentum=0.6', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 3, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'][:-1], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('NGD. momentum=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/NGD/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2721638",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf26acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "# kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9bb307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer, kernel_initializer, bias_initializer)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "    \n",
    "        \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "458ec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# print(\"\\n\\n*********** AdaGrad ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6315fcaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# plt.subplots(1, 1, figsize=((9, 6)))\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'][:-1], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('AdaGrad', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/AdaGrad/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c07eb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24e1216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches = np.array_split(x_train, 40)\n",
    "# y_train_batches = np.array_split(x_train, 40)\n",
    "\n",
    "# # x_val_batches = np.array_split(x_val, 61)\n",
    "# # y_val_batches = np.array_split(y_val, 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae6273e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "\n",
    "# kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# model = build_model(optimizer, kernel_initializer, bias_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa570643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 500\n",
    "\n",
    "# weights_1 = []\n",
    "# weights_2 = []\n",
    "# weights_3 = []\n",
    "# weights_4 = []\n",
    "\n",
    "# gradients = []\n",
    "# all_weights = []\n",
    "\n",
    "# loss_values = []\n",
    "# val_loss_values = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "#     all_weights.append([])\n",
    "\n",
    "#     # Iterate over the batches of the dataset.   \n",
    "#     for step, (x_batch_train, y_batch_train) in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "#         weights_1.append(model.layers[1].get_weights())\n",
    "#         weights_2.append(model.layers[2].get_weights())\n",
    "#         weights_3.append(model.layers[3].get_weights())\n",
    "#         weights_4.append(model.layers[4].get_weights())\n",
    "\n",
    "#         for i in range(1, 5):\n",
    "#             all_weights[epoch].append(model.layers[i].get_weights())\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             logits = model(tf.constant(x_batch_train, shape=(len(x_batch_train),11)))\n",
    "#             loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "#         grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "#         loss_values.append(loss_value)\n",
    "#         gradients.append(grads)\n",
    "\n",
    "#     # Run a validation loop at the end of each epoch.\n",
    "#     val_logits = model(x_val, training=False)\n",
    "#     loss_value_valid = loss_fn(y_val, val_logits)\n",
    "#     val_loss_values.append(loss_value_valid)\n",
    "# #     print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "# avg_global_speeds = list()\n",
    "# max_global_speeds = list()\n",
    "# min_global_speeds = list()\n",
    "# for epoch in range(epochs-1): \n",
    "#     avg_speeds = list()\n",
    "#     max_speeds = list()\n",
    "#     min_speeds = list()\n",
    "\n",
    "#     for layer in range(5): \n",
    "#         try:\n",
    "#             speed = (all_weights[epoch+1][layer][0] - all_weights[epoch][layer][0])/gradients[epoch][layer*2][0]\n",
    "#         except Exception as ex:\n",
    "#             print(f'layer = {layer}, i = {epoch}')\n",
    "#             print('--------------------------------------')\n",
    "#             print(len(all_weights[epoch+1][layer][0]))\n",
    "#             print(len(all_weights[epoch][layer][0]))\n",
    "#             print('--------------------------------------')\n",
    "#             print(all_weights[epoch+1][layer][0])\n",
    "#             print('--------------------------------------')\n",
    "#             print(all_weights[epoch][layer][0])\n",
    "#             print('--------------------------------------')\n",
    "#             print(gradients[epoch][layer])\n",
    "#             print('--------------------------------------')\n",
    "#             print(ex)\n",
    "#         avg_speeds.append(tf.get_static_value(tf.reduce_mean(tf.reduce_mean(tf.abs(speed), axis = 0), axis = 0)))\n",
    "#         max_speeds.append(tf.get_static_value(tf.math.reduce_max(tf.math.reduce_max(tf.abs(speed), axis =0), axis = 0)))\n",
    "#         min_speeds.append(tf.get_static_value(tf.math.reduce_min(tf.math.reduce_max(tf.abs(speed), axis =0), axis = 0)))\n",
    "#     avg_global_speeds.append(sum(avg_speeds)/ len(avg_speeds))\n",
    "#     max_global_speeds.append(max(max_speeds))\n",
    "#     min_global_speeds.append(min(min_speeds))\n",
    "\n",
    "# speeds_0 = list()\n",
    "# for i in range(epochs-1): \n",
    "#     speeds_0.append(tf.get_static_value(tf.abs((weights_1[i+1][0][0][0] - weights_1[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "# speeds_1 = list()\n",
    "# for i in range(epochs-1): \n",
    "#     speeds_1.append(tf.get_static_value(tf.abs((weights_2[i+1][0][0][0] - weights_2[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "# speeds_2 = list()\n",
    "# for i in range(epochs-1): \n",
    "#     speeds_2.append(tf.get_static_value(tf.abs((weights_3[i+1][0][0][0] - weights_3[i][0][0][0])/gradients[i][2][0][0])))\n",
    "\n",
    "# speeds_3 = list()\n",
    "# for i in range(epochs-1): \n",
    "#     speeds_3.append(tf.get_static_value(tf.abs((weights_4[i+1][0][0][0] - weights_4[i][0][0][0])/gradients[i][2][0][0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "866f4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs_list = [_ for _ in range(epochs-1)]\n",
    "\n",
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(9, 6))\n",
    "\n",
    "# ax[0][0].plot(epochs_list, speeds_0, label = 'Training loss')\n",
    "\n",
    "# ax[0][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][0].set_title(f'Первый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][0].set_xlim(0, 500)\n",
    "# ax[0][0].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "\n",
    "# ax[0][1].plot(epochs_list, speeds_1, label = 'Training loss')\n",
    "\n",
    "# ax[0][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][1].set_title(f'Второй слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][1].set_xlim(0, 500)\n",
    "# ax[0][1].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "\n",
    "# ax[1][0].plot(epochs_list, speeds_2, label = 'Training loss')\n",
    "\n",
    "# ax[1][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][0].set_title(f'Третий слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][0].set_xlim(0, 500)\n",
    "# ax[1][0].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "\n",
    "# ax[1][1].plot(epochs_list, speeds_2, label = 'Training loss')\n",
    "\n",
    "# ax[1][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][1].set_title(f'Четвертый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][1].set_xlim(0, 500)\n",
    "# ax[1][1].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/AdaGrad/Speeds.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2820b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "# ax[0].plot(epochs_list, avg_global_speeds, label = 'Training loss')\n",
    "\n",
    "# ax[0].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[0].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[0].set_title(f'Средняя скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[0].set_xlim(0, 100)\n",
    "# # plt.ylim(0, 10)\n",
    "# ax[0].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "# ax[1].plot(epochs_list, max_global_speeds, label = 'Training loss')\n",
    "\n",
    "# ax[1].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[1].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[1].set_title(f'Максимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[1].set_xlim(0, 100)\n",
    "# # plt.ylim(0, 10)\n",
    "# ax[1].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "# ax[2].plot(epochs_list, min_global_speeds, label = 'Training loss')\n",
    "\n",
    "# ax[2].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[2].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[2].set_title(f'Минимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[2].set_xlim(0, 100)\n",
    "# # ax[2].ylim(0, 10)\n",
    "# ax[2].tick_params(labelsize=tick_labelsize) \n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/AdaGrad/min_max_mean.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287d2d3",
   "metadata": {},
   "source": [
    "### RMSProp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e7fa41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "\n",
    "# tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0)\n",
    "# optimizer2 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.3)\n",
    "# optimizer3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.6)\n",
    "# optimizer4 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff6b5a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d23301e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer2 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer2   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2, kernel_initializer2, bias_initializer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fcb0ca89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer3 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer3   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3, kernel_initializer3, bias_initializer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d86a9947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer4 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer4   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4, kernel_initializer4, bias_initializer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6ab54ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** RMSProp. rho=0 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** RMSProp. rho=0.3 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** RMSProp. rho=0.6 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** RMSProp. rho=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89d796da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('RMSProp. rho=0', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'][:-1], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('RMSProp. rho=0.3', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('RMSProp. rho=0.6', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('RMSProp. rho=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/RMSProp/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca39977a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizer1 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0)\n",
    "# optimizer2 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.3)\n",
    "# optimizer3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.6)\n",
    "# optimizer4 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]\n",
    "\n",
    "# # kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# # bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# # errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# # model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# speeds = []\n",
    "\n",
    "# epochs = 200\n",
    "# epochs_list = [_ for _ in range(epochs-1)]\n",
    "\n",
    "# for optimizer in optimizers:\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    " \n",
    "#     kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "#     bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "#     model = build_model(optimizer, kernel_initializer, bias_initializer)\n",
    "\n",
    "#     speeds_i = weights_speed(epochs, model, loss_fn)\n",
    "#     speeds.append(speeds_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9908ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(9, 6))\n",
    "\n",
    "# ax[0][0].plot(epochs_list, speeds[0][0], label = 'rho=0')\n",
    "# ax[0][0].plot(epochs_list, speeds[1][0], label = 'rho=0.3')\n",
    "# ax[0][0].plot(epochs_list, speeds[2][0], label = 'rho=0.6')\n",
    "# ax[0][0].plot(epochs_list, speeds[3][0], label = 'rho=0.9')\n",
    "\n",
    "# ax[0][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][0].set_title(f'RMSProp. Первый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][0].set_xlim(0, 200)\n",
    "# ax[0][0].tick_params(labelsize=tick_labelsize)\n",
    "# ax[0][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[0][1].plot(epochs_list, speeds[0][1], label = 'rho=0')\n",
    "# ax[0][1].plot(epochs_list, speeds[1][1], label = 'rho=0.3')\n",
    "# ax[0][1].plot(epochs_list, speeds[2][1], label = 'rho=0.6')\n",
    "# ax[0][1].plot(epochs_list, speeds[3][1], label = 'rho=0.9')\n",
    "\n",
    "# ax[0][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][1].set_title(f'RMSProp. Второй слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][1].set_xlim(0, 200)\n",
    "# ax[0][1].tick_params(labelsize=tick_labelsize) \n",
    "# ax[0][1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[1][0].plot(epochs_list, speeds[0][2], label = 'rho=0')\n",
    "# ax[1][0].plot(epochs_list, speeds[1][2], label = 'rho=0.3')\n",
    "# ax[1][0].plot(epochs_list, speeds[2][2], label = 'rho=0.6')\n",
    "# ax[1][0].plot(epochs_list, speeds[3][2], label = 'rho=0.9')\n",
    "\n",
    "# ax[1][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][0].set_title(f'RMSProp. Третий слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][0].set_xlim(0, 200)\n",
    "# ax[1][0].tick_params(labelsize=tick_labelsize) \n",
    "# ax[1][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[1][1].plot(epochs_list, speeds[0][3], label = 'rho=0')\n",
    "# ax[1][1].plot(epochs_list, speeds[1][3], label = 'rho=0.3')\n",
    "# ax[1][1].plot(epochs_list, speeds[2][3], label = 'rho=0.6')\n",
    "# ax[1][1].plot(epochs_list, speeds[3][3], label = 'rho=0.9')\n",
    "\n",
    "# ax[1][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][1].set_title(f'RMSProp. Четвертый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][1].set_xlim(0, 200)\n",
    "# ax[1][1].tick_params(labelsize=tick_labelsize) \n",
    "# ax[1][1].legend(fontsize=fontsize)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/RMSProp/Speeds.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f93de212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "# ax[0].plot(epochs_list, speeds[0][-1], label = 'rho=0')\n",
    "# ax[0].plot(epochs_list, speeds[1][-1], label = 'rho=0.3')\n",
    "# ax[0].plot(epochs_list, speeds[2][-1], label = 'rho=0.6')\n",
    "# ax[0].plot(epochs_list, speeds[3][-1], label = 'rho=0.9')\n",
    "\n",
    "# ax[0].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[0].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[0].set_title(f'RMSProp. Средняя скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[0].set_xlim(0, 100)\n",
    "# ax[0].set_ylim(0, 4)\n",
    "# ax[0].tick_params(labelsize=tick_labelsize) \n",
    "# ax[0].legend(fontsize=fontsize)\n",
    "\n",
    "# ax[1].plot(epochs_list, speeds[0][0], label = 'rho=0')\n",
    "# ax[1].plot(epochs_list, speeds[1][0], label = 'rho=0.3')\n",
    "# ax[1].plot(epochs_list, speeds[2][0], label = 'rho=0.6')\n",
    "# ax[1].plot(epochs_list, speeds[3][0], label = 'rho=0.9')\n",
    "\n",
    "# ax[1].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[1].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[1].set_title(f'RMSProp. Максимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[1].set_xlim(0, 100)\n",
    "# ax[1].set_ylim(0, 0.4)\n",
    "# ax[1].tick_params(labelsize=tick_labelsize) \n",
    "# ax[1].legend(fontsize=fontsize)\n",
    "\n",
    "# ax[2].plot(epochs_list, speeds[0][1], label = 'rho=0')\n",
    "# ax[2].plot(epochs_list, speeds[1][1], label = 'rho=0.3')\n",
    "# ax[2].plot(epochs_list, speeds[2][1], label = 'rho=0.6')\n",
    "# ax[2].plot(epochs_list, speeds[3][1], label = 'rho=0.9')\n",
    "\n",
    "# ax[2].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[2].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[2].set_title(f'RMSProp. Минимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[2].set_xlim(0, 100)\n",
    "# ax[2].set_ylim(0, 0.4)\n",
    "# ax[2].tick_params(labelsize=tick_labelsize) \n",
    "# ax[2].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/RMSProp/min_max_mean.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb528ec",
   "metadata": {},
   "source": [
    "### AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "238723a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "\n",
    "# tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0)\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0)\n",
    "# optimizer2 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.3)\n",
    "# optimizer3 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.6)\n",
    "# optimizer4 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58b1f450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2f1e6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer2 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer2   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2, kernel_initializer2, bias_initializer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c15b4b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer3 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer3   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3, kernel_initializer3, bias_initializer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c322f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_initializer4 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer4   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4, kernel_initializer4, bias_initializer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b9dacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** AdaDelta. rho=0 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** AdaDelta. rho=0.3 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** AdaDelta. rho=0.6 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** AdaDelta. rho=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5b02be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('AdaDelta. rho=0', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('AdaDelta. rho=0.3', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'][:-1], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('AdaDelta. rho=0.6', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('AdaDelta. rho=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/AdaDelta/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ee41221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizer1 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0)\n",
    "# optimizer2 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.3)\n",
    "# optimizer3 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.6)\n",
    "# optimizer4 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]\n",
    "\n",
    "# # kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# # bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# # errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# # model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# speeds = []\n",
    "\n",
    "# epochs = 200\n",
    "# epochs_list = [_ for _ in range(epochs-1)]\n",
    "\n",
    "# for optimizer in optimizers:\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    " \n",
    "#     kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "#     bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "#     model = build_model(optimizer, kernel_initializer, bias_initializer)\n",
    "\n",
    "#     speeds_i = weights_speed(epochs, model, loss_fn)\n",
    "#     speeds.append(speeds_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40c5c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(9, 6))\n",
    "\n",
    "# ax[0][0].plot(epochs_list, speeds[0][0], label = 'rho=0')\n",
    "# ax[0][0].plot(epochs_list, speeds[1][0], label = 'rho=0.3')\n",
    "# ax[0][0].plot(epochs_list, speeds[2][0], label = 'rho=0.6')\n",
    "# ax[0][0].plot(epochs_list, speeds[3][0], label = 'rho=0.9')\n",
    "\n",
    "# ax[0][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][0].set_title(f'AdaDelta. Первый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][0].set_xlim(0, 200)\n",
    "# ax[0][0].tick_params(labelsize=tick_labelsize)\n",
    "# ax[0][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[0][1].plot(epochs_list, speeds[0][1], label = 'rho=0')\n",
    "# ax[0][1].plot(epochs_list, speeds[1][1], label = 'rho=0.3')\n",
    "# ax[0][1].plot(epochs_list, speeds[2][1], label = 'rho=0.6')\n",
    "# ax[0][1].plot(epochs_list, speeds[3][1], label = 'rho=0.9')\n",
    "\n",
    "# ax[0][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][1].set_title(f'AdaDelta. Второй слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][1].set_xlim(0, 200)\n",
    "# ax[0][1].tick_params(labelsize=tick_labelsize)\n",
    "# ax[0][1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "# ax[1][0].plot(epochs_list, speeds[0][2], label = 'rho=0')\n",
    "# ax[1][0].plot(epochs_list, speeds[1][2], label = 'rho=0.3')\n",
    "# ax[1][0].plot(epochs_list, speeds[2][2], label = 'rho=0.6')\n",
    "# ax[1][0].plot(epochs_list, speeds[3][2], label = 'rho=0.9')\n",
    "\n",
    "# ax[1][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][0].set_title(f'AdaDelta. Третий слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][0].set_xlim(0, 200)\n",
    "# ax[1][0].tick_params(labelsize=tick_labelsize)\n",
    "# ax[1][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "# ax[1][1].plot(epochs_list, speeds[0][3], label = 'rho=0')\n",
    "# ax[1][1].plot(epochs_list, speeds[1][3], label = 'rho=0.3')\n",
    "# ax[1][1].plot(epochs_list, speeds[2][3], label = 'rho=0.6')\n",
    "# ax[1][1].plot(epochs_list, speeds[3][3], label = 'rho=0.9')\n",
    "\n",
    "# ax[1][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][1].set_title(f'AdaDelta. Четвертый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][1].set_xlim(0, 200)\n",
    "# ax[1][1].tick_params(labelsize=tick_labelsize)\n",
    "# ax[1][1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/AdaDelta/Speeds.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87674c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "# ax[0].plot(epochs_list, speeds[0][-1], label = 'rho=0')\n",
    "# ax[0].plot(epochs_list, speeds[1][-1], label = 'rho=0.3')\n",
    "# ax[0].plot(epochs_list, speeds[2][-1], label = 'rho=0.6')\n",
    "# ax[0].plot(epochs_list, speeds[3][-1], label = 'rho=0.9')\n",
    "\n",
    "# ax[0].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[0].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[0].set_title(f'AdaDelta. Средняя скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[0].set_xlim(0, 100)\n",
    "# ax[0].set_ylim(0, 0.06)\n",
    "# ax[0].tick_params(labelsize=tick_labelsize) \n",
    "# ax[0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[1].plot(epochs_list, speeds[0][0], label = 'rho=0')\n",
    "# ax[1].plot(epochs_list, speeds[1][0], label = 'rho=0.3')\n",
    "# ax[1].plot(epochs_list, speeds[2][0], label = 'rho=0.6')\n",
    "# ax[1].plot(epochs_list, speeds[3][0], label = 'rho=0.9')\n",
    "\n",
    "# ax[1].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[1].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[1].set_title(f'AdaDelta. Максимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[1].set_xlim(0, 100)\n",
    "# ax[1].set_ylim(0, 0.00005)\n",
    "# ax[1].tick_params(labelsize=tick_labelsize) \n",
    "# ax[1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[2].plot(epochs_list, speeds[0][1], label = 'rho=0')\n",
    "# ax[2].plot(epochs_list, speeds[1][1], label = 'rho=0.3')\n",
    "# ax[2].plot(epochs_list, speeds[2][1], label = 'rho=0.6')\n",
    "# ax[2].plot(epochs_list, speeds[3][1], label = 'rho=0.9')\n",
    "\n",
    "# ax[2].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[2].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[2].set_title(f'AdaDelta. Минимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[2].set_xlim(0, 100)\n",
    "# ax[2].set_ylim(0, 0.00005)\n",
    "# ax[2].tick_params(labelsize=tick_labelsize) \n",
    "# ax[2].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/AdaDelta/min_max_mean.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2cf8a",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c34bffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "\n",
    "# tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.1, beta_2=0.001)\n",
    "# optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.1, beta_2=0.1)\n",
    "# optimizer3 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.9)\n",
    "# optimizer4 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.5)\n",
    "# optimizer5 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9)\n",
    "# optimizer6 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c70ec753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcd67c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer2 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer2   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors2  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model2 = build_model(optimizer2, kernel_initializer2, bias_initializer2)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist2 = model2.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors2['epochs'].append(epochs)\n",
    "    \n",
    "#     errors2['train']. append(hist2.history['loss'][0])\n",
    "#     errors2['val'].   append(hist2.history['val_loss'][0])\n",
    "#     errors2['test'].  append(model2.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist2.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist2.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06919a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer3 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer3   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors3  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model3 = build_model(optimizer3, kernel_initializer3, bias_initializer3)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist3 = model3.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors3['epochs'].append(epochs)\n",
    "    \n",
    "#     errors3['train']. append(hist3.history['loss'][0])\n",
    "#     errors3['val'].   append(hist3.history['val_loss'][0])\n",
    "#     errors3['test'].  append(model3.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist3.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist3.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7228d29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer4 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer4   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors4  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model4 = build_model(optimizer4, kernel_initializer4, bias_initializer4)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist4 = model4.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors4['epochs'].append(epochs)\n",
    "    \n",
    "#     errors4['train']. append(hist4.history['loss'][0])\n",
    "#     errors4['val'].   append(hist4.history['val_loss'][0])\n",
    "#     errors4['test'].  append(model4.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist4.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist4.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebcbd304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer5 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer5   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors5  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model5 = build_model(optimizer5, kernel_initializer5, bias_initializer5)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist5 = model5.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors5['epochs'].append(epochs)\n",
    "    \n",
    "#     errors5['train']. append(hist5.history['loss'][0])\n",
    "#     errors5['val'].   append(hist5.history['val_loss'][0])\n",
    "#     errors5['test'].  append(model5.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist5.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist5.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b684f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer6 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer6   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors6  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model6 = build_model(optimizer5, kernel_initializer6, bias_initializer6)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 2000 or val_cce > 0:\n",
    "    \n",
    "#     hist6 = model6.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors6['epochs'].append(epochs)\n",
    "    \n",
    "#     errors6['train']. append(hist6.history['loss'][0])\n",
    "#     errors6['val'].   append(hist6.history['val_loss'][0])\n",
    "#     errors6['test'].  append(model6.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist6.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist6.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e52dbeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score5 = model5.evaluate(x_test, y_test, verbose=0)\n",
    "# test_score6 = model6.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.1, beta_2=0.001 ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.1, beta_2=0.1 ***********\")\n",
    "# print(f'Epochs: ', len(errors2['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist2.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist2.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist2.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist2.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score2[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.5, beta_2=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors3['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist3.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist3.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist3.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist3.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score3[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score3[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.9, beta_2=0.5 ***********\")\n",
    "# print(f'Epochs: ', len(errors4['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist4.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist4.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist4.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist4.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score4[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score4[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.9, beta_2=0.9 ***********\")\n",
    "# print(f'Epochs: ', len(errors5['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist5.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist5.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist5.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist5.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score5[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score5[1]))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\\n*********** Adam. beta_1=0.9, beta_2=0.999 ***********\")\n",
    "# print(f'Epochs: ', len(errors6['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist6.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist6.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist6.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist6.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score6[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score6[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7dc8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(2, 3, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'][:-1], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.1, beta_2=0.001', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-2\n",
    "# plt.subplot(2, 3, 2)\n",
    "# plt.plot(errors2['epochs'], errors2['train'], label='train')\n",
    "# plt.plot(errors2['epochs'], errors2['val'],   label='val')\n",
    "# plt.plot(errors2['epochs'][:-1], errors2['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.1, beta_2=0.1', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-3\n",
    "# plt.subplot(2, 3, 3)\n",
    "# plt.plot(errors3['epochs'], errors3['train'], label='train')\n",
    "# plt.plot(errors3['epochs'], errors3['val'],   label='val')\n",
    "# plt.plot(errors3['epochs'], errors3['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.5, beta_2=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # Model-4\n",
    "# plt.subplot(2, 3, 4)\n",
    "# plt.plot(errors4['epochs'], errors4['train'], label='train')\n",
    "# plt.plot(errors4['epochs'], errors4['val'],   label='val')\n",
    "# plt.plot(errors4['epochs'][:-1], errors4['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.9, beta_2=0.5', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "# # Model-5\n",
    "# plt.subplot(2, 3, 5)\n",
    "# plt.plot(errors5['epochs'], errors5['train'], label='train')\n",
    "# plt.plot(errors5['epochs'], errors5['val'],   label='val')\n",
    "# plt.plot(errors5['epochs'], errors5['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.9, beta_2=0.9', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "# # Model-6\n",
    "# plt.subplot(2, 3, 6)\n",
    "# plt.plot(errors6['epochs'], errors6['train'], label='train')\n",
    "# plt.plot(errors6['epochs'], errors6['val'],   label='val')\n",
    "# plt.plot(errors6['epochs'], errors6['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('Adam. beta_1=0.9, beta_2=0.999', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/Adam/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "36d93a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bf64fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches = np.array_split(x_train, 40)\n",
    "# y_train_batches = np.array_split(x_train, 40)\n",
    "\n",
    "# # x_val_batches = np.array_split(x_val, 61)\n",
    "# # y_val_batches = np.array_split(y_val, 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "daf9a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # метод оптимизации\n",
    "\n",
    "# tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.1, beta_2=0.001)\n",
    "# optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.1, beta_2=0.1)\n",
    "# optimizer3 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.9)\n",
    "# optimizer4 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.5)\n",
    "# optimizer5 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9)\n",
    "# optimizer6 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# optimizers = [optimizer1, optimizer2, optimizer3, optimizer4, optimizer5, optimizer6]\n",
    "\n",
    "# # kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# # bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# # errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# # model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# speeds = []\n",
    "\n",
    "# epochs = 200\n",
    "# epochs_list = [_ for _ in range(epochs-1)]\n",
    "\n",
    "# for optimizer in optimizers:\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    " \n",
    "#     kernel_initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "#     bias_initializer   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "#     model = build_model(optimizer, kernel_initializer, bias_initializer)\n",
    "\n",
    "#     speeds_i = weights_speed(epochs, model, loss_fn)\n",
    "#     speeds.append(speeds_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f333488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(9, 6))\n",
    "\n",
    "# ax[0][0].plot(epochs_list, speeds[0][0], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[0][0].plot(epochs_list, speeds[1][0], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[0][0].plot(epochs_list, speeds[2][0], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[0][0].plot(epochs_list, speeds[3][0], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[0][0].plot(epochs_list, speeds[4][0], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[0][0].plot(epochs_list, speeds[5][0], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[0][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][0].set_title(f'Adam. Первый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][0].set_xlim(0, 200)\n",
    "# ax[0][0].tick_params(labelsize=tick_labelsize)\n",
    "# ax[0][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[0][1].plot(epochs_list, speeds[0][1], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[0][1].plot(epochs_list, speeds[1][1], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[0][1].plot(epochs_list, speeds[2][1], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[0][1].plot(epochs_list, speeds[3][1], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[0][1].plot(epochs_list, speeds[4][1], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[0][1].plot(epochs_list, speeds[5][1], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[0][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[0][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[0][1].set_title(f'Adam. Второй слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[0][1].set_xlim(0, 200)\n",
    "# ax[0][1].tick_params(labelsize=tick_labelsize)\n",
    "# ax[0][1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "# ax[1][0].plot(epochs_list, speeds[0][2], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[1][0].plot(epochs_list, speeds[1][2], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[1][0].plot(epochs_list, speeds[2][2], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[1][0].plot(epochs_list, speeds[3][2], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[1][0].plot(epochs_list, speeds[4][2], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[1][0].plot(epochs_list, speeds[5][2], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[1][0].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][0].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][0].set_title(f'Adam. Третий слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][0].set_xlim(0, 200)\n",
    "# ax[1][0].tick_params(labelsize=tick_labelsize)\n",
    "# ax[1][0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[1][1].plot(epochs_list, speeds[0][3], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[1][1].plot(epochs_list, speeds[1][3], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[1][1].plot(epochs_list, speeds[2][3], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[1][1].plot(epochs_list, speeds[3][3], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[1][1].plot(epochs_list, speeds[4][3], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[1][1].plot(epochs_list, speeds[5][3], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[1][1].set_xlabel('Epochs', fontsize=fontsize)\n",
    "# ax[1][1].set_ylabel('Speed', fontsize=fontsize)\n",
    "# ax[1][1].set_title(f'Adam. Четвертый слой, первый нейрон', fontsize=fontsize)\n",
    "\n",
    "# ax[1][1].set_xlim(0, 200)\n",
    "# ax[1][1].tick_params(labelsize=tick_labelsize)\n",
    "# ax[1][1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/Adam/Speeds.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "327bbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "\n",
    "# fontsize = 8\n",
    "# tick_labelsize = 6\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "# ax[0].plot(epochs_list, speeds[0][-1], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[0].plot(epochs_list, speeds[1][-1], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[0].plot(epochs_list, speeds[2][-1], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[0].plot(epochs_list, speeds[3][-1], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[0].plot(epochs_list, speeds[4][-1], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[0].plot(epochs_list, speeds[5][-1], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "\n",
    "# ax[0].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[0].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[0].set_title(f'Adam. Средняя скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[0].set_xlim(0, 100)\n",
    "# ax[0].set_ylim(0, 8)\n",
    "# ax[0].tick_params(labelsize=tick_labelsize) \n",
    "# ax[0].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[1].plot(epochs_list, speeds[0][0], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[1].plot(epochs_list, speeds[1][0], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[1].plot(epochs_list, speeds[2][0], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[1].plot(epochs_list, speeds[3][0], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[1].plot(epochs_list, speeds[4][0], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[1].plot(epochs_list, speeds[5][0], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[1].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[1].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[1].set_title(f'Adam. Максимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[1].set_xlim(0, 100)\n",
    "# ax[1].set_ylim(0, 1)\n",
    "# ax[1].tick_params(labelsize=tick_labelsize) \n",
    "# ax[1].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# ax[2].plot(epochs_list, speeds[0][1], label = 'beta_1=0.1, beta_2=0.001')\n",
    "# ax[2].plot(epochs_list, speeds[1][1], label = 'beta_1=0.1, beta_2=0.1')\n",
    "# ax[2].plot(epochs_list, speeds[2][1], label = 'beta_1=0.5, beta_2=0.9')\n",
    "# ax[2].plot(epochs_list, speeds[3][1], label = 'beta_1=0.9, beta_2=0.5')\n",
    "# ax[2].plot(epochs_list, speeds[4][1], label = 'beta_1=0.9, beta_2=0.9')\n",
    "# ax[2].plot(epochs_list, speeds[5][1], label = 'beta_1=0.9, beta_2=0.999')\n",
    "\n",
    "# ax[2].set_xlabel('Epochs',  fontsize=fontsize)\n",
    "# ax[2].set_ylabel('Speed',  fontsize=fontsize)\n",
    "# ax[2].set_title(f'Adam. Минимальная скорость',  fontsize=fontsize)\n",
    "\n",
    "# ax[2].set_xlim(0, 100)\n",
    "# ax[2].set_ylim(0, 0.75)\n",
    "# ax[2].tick_params(labelsize=tick_labelsize) \n",
    "# ax[2].legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'./Graphs/Adam/min_max_mean.jpg')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728e89",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ba35ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # метод оптимизации\n",
    "# optimizer1 = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d070697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel_initializer1 = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "# bias_initializer1   = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# errors1  = {'epochs': [], 'train':[], 'val':[], 'test':[]}\n",
    "# model1 = build_model(optimizer1, kernel_initializer1, bias_initializer1)\n",
    "\n",
    "# val_cce = 1000\n",
    "# epochs = 0\n",
    "\n",
    "# while epochs < 100 or val_cce > 0:\n",
    "    \n",
    "#     hist1 = model1.fit(x=x_train, y=y_train, \n",
    "#                       epochs=1, \n",
    "#                       validation_split= val_part,  \n",
    "#                       batch_size=50)\n",
    "     \n",
    "#     epochs += 1\n",
    "#     print('Epoches: ', epochs)\n",
    "    \n",
    "#     errors1['epochs'].append(epochs)\n",
    "    \n",
    "#     errors1['train']. append(hist1.history['loss'][0])\n",
    "#     errors1['val'].   append(hist1.history['val_loss'][0])\n",
    "#     errors1['test'].  append(model1.evaluate(x_test, y_test, verbose=0)[0])\n",
    "    \n",
    "#     if hist1.history['val_loss'][0] < val_cce:\n",
    "#         val_cce = hist1.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "761fd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# print(\"\\n\\n*********** SGD ***********\")\n",
    "# print(f'Epochs: ', len(errors1['epochs']))\n",
    "# print('\\n' + \"| Train |\")\n",
    "# print('СCE: {:.4f}'.format(hist1.history['loss'][0]),                       '\\n' +\n",
    "#       'MAE: {:.4f}'.format(hist1.history['mean_absolute_error'][0]),        '\\n' +\n",
    "#       'val_CCE: {:.4f}'.format(hist1.history['val_loss'][0]),               '\\n' +\n",
    "#       'val_MAE: {:.4f}'.format(hist1.history['val_mean_absolute_error'][0]))\n",
    "      \n",
    "# print('\\n' + \"| Test |\")\n",
    "# print('СCE: {:.4f}'.format(test_score1[0]),                                 '\\n' +\n",
    "#       'MAE: {:.4f}'.format(test_score1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f800d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(9, 6))\n",
    "\n",
    "# fontsize = 14\n",
    "# tick_labelsize = 12\n",
    "\n",
    "# # Model-1\n",
    "# plt.subplot(1, 1, 1)\n",
    "# plt.plot(errors1['epochs'], errors1['train'], label='train')\n",
    "# plt.plot(errors1['epochs'], errors1['val'],   label='val')\n",
    "# plt.plot(errors1['epochs'], errors1['test'],  label='test')\n",
    "\n",
    "# plt.xlabel('Epochs', fontsize=fontsize)\n",
    "# plt.ylabel('E(t)',   fontsize=fontsize)\n",
    "\n",
    "# plt.title('SGD', fontsize=fontsize)\n",
    "# plt.tick_params(labelsize=tick_labelsize) \n",
    "# plt.legend(fontsize=fontsize)\n",
    "\n",
    "  \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"./Graphs/SGD/Errors_1.jpg\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56dca3",
   "metadata": {},
   "source": [
    "### Fletcher-Reevse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ca39e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neupy import algorithms, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3aed06a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'connection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-316479e8f6ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m optimizer = algorithms.ConjugateGradient(\n\u001b[0m\u001b[0;32m      2\u001b[0m     network=[\n\u001b[0;32m      3\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'connection'"
     ]
    }
   ],
   "source": [
    "optimizer = algorithms.ConjugateGradient(\n",
    "    network=[\n",
    "        layers.Tanh(13),\n",
    "        layers.Tanh(50),\n",
    "        layers.Tanh(1)\n",
    "    ],\n",
    "    update_function='fletcher_reeves',\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25268e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Tanh(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838416f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
